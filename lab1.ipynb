{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 1: Optimizing Generative AI Workloads with Embedded Arm Processors**\n",
    "\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "Welcome to the **Optimizing Generative AI Workloads with Embedded Arm Processors** lab! In this hands-on session, you will explore how Armv8-A advanced vector processing capabilities can significantly accelerate computationally intensive tasks commonly found in artificial intelligence (AI) applications. By the end of this lab, you will gain a deep understanding of how low-level optimizations can enhance performance and how to leverage these optimizations within high-level AI frameworks like PyTorch and lightweight inference libraries such as llama.cpp.\n",
    "\n",
    "Specifically, you will learn how to optimize all layers of the stack, from low-level matrix multiplication in C using Arm's vector processing instructions to implementing weight-only quantization from scratch. You will demonstrate the deployment and optimization of the **OpenELM model** using llama.cpp, a state-of-the-art framework for efficient large language model inference on resource-constrained devices. This practical approach will showcase how to run modern generative AI workloads effectively on Arm hardware while minimizing latency and memory usage.\n",
    "\n",
    "**Requirements**: To complete this lab, you will need an Armv8-A 64-bit system running a Linux-based operating system, such as a Raspberry Pi 4 or 5. The lab has been thoroughly tested on the Raspberry Pi 5 for compatibility and performance.\n",
    "\n",
    "### **Why Arm Vector Instructions?**\n",
    "\n",
    "Arm processors are ubiquitous in modern computing, powering everything from smartphones to edge devices and increasingly, servers and supercomputers. Their architecture is designed for energy efficiency and performance, making them ideal for deploying AI models in diverse environments. One of the key features that enable Arm processors to excel in AI workloads is their support for **vector instructions**, such as Neon and i8mm, which allow for parallel processing of multiple data points in a single instruction cycle. Armv8-A is a member of the Arm architecture family, representing the 8th generation of Arm's advanced architecture with a focus on 64-bit computing, high performance, and scalable designs. It builds upon the energy-efficient foundation of earlier Arm architectures while introducing advanced features such as support for AArch64 (the 64-bit instruction set), enhanced vector processing capabilities, and improved cryptographic extensions. Armv8-A is widely adopted in a range of mobile and embedded devices, including the **Raspberry Pi 4/5**.\n",
    "\n",
    "### **Lab Objectives**\n",
    "\n",
    "The objectives of this lab are as follows:\n",
    "\n",
    "1. **Matrix Multiplication Optimization**:\n",
    "   - Analyze three C-based matrix multiplication implementations (`naive`, `fp32_neon`, and `int8_neon`) to understand their performance differences.\n",
    "   - Utilize Armv8's Neon vector processing capability to accelerate matrix multiplications, a fundamental operation of AI workloads.\n",
    "\n",
    "2. **Benchmark PyTorch Operations**:\n",
    "   - Measure the performance of PyTorch's matrix multiplication operations in different precisions.\n",
    "   - Examine the generated assembly code to identify how PyTorch accelerates matrix multiplications.\n",
    "\n",
    "3. **Run Inference on a Language Model**:\n",
    "   - Load and run inference on the OpenELM model using llama.cpp.\n",
    "   - Explore its computational graph to understand the underlying operations.\n",
    "\n",
    "4. **Apply Integer Quantization**:\n",
    "   - Learn how integer quantization can reduce model size and accelerate inference.\n",
    "   - Implement and evaluate quantization on matrix multiplication operations.\n",
    "   - Apply integer quantization to the OpenELM model and record its latency speedups and memory reductions.\n",
    "\n",
    "5. **Deploy and Optimize llama.cpp on Arm**:\n",
    "   - Understand the advantages of llama.cpp for running large language models on resource-constrained devices.\n",
    "   - Set up, compile, and benchmark llama.cpp on Arm, including its utilization of quantized matrix multiplication.\n",
    "   - Analyze the memory and latency trade-offs of llama.cpp compared to traditional frameworks like PyTorch.\n",
    "\n",
    "### **What You Will Learn**\n",
    "\n",
    "- **Low-Level Optimizations**: Understand how Arm's Neon SIMD instructions can accelerate matrix computations.\n",
    "- **Performance Benchmarking**: Develop skills to benchmark operations effectively and analyze performance trade-offs.\n",
    "- **Framework Integration**: Explore how low-level optimizations can complement high-level frameworks like PyTorch.\n",
    "- **Quantization Techniques**: Learn to implement quantization for efficient AI workloads on resource-constrained devices.\n",
    "- **Edge Deployment**: Practical deployment of llama.cpp for language model inference.\n",
    "\n",
    "### **Prerequisites**\n",
    "\n",
    "To follow this lab, you should have a basic understanding of:\n",
    "\n",
    "- **C Programming**: Familiarity with C syntax and basic memory management.\n",
    "- **Python Programming**: Experience writing and running Python scripts.\n",
    "- **Matrix Operations**: A general understanding of matrix multiplication.\n",
    "- **Knowledge of SIMD (Single Instruction, Multiple Data) and Transformer models**: This is covered in Chapter 2 and 4. \n",
    "- **Go through the first step of section 3**: This step, involves setting up a Hugging Face account, and downloading the OpenELM LLM. This can take around half an hour to complete.\n",
    "- **The lab depends upon Linux tools:** python, perf, cmake, curl, g++\n",
    "and python packages: PyTorch, huggingface-cli, transformers.\n",
    "\n",
    "**Note:** It is assumed that the same Jupyter Kernel, is used throughout this lab and that all cells are run sequentially\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Matrix Multiplication Optimization**\n",
    "\n",
    "This lab begins with an exploration of matrix multiplication operators, the core computational components of AI workloads. Understanding and optimizing these operations can significantly enhance the inference performance of generative AI models. In this section, we will implement and compare three matrix multiplication approaches. While they are mathematically identical, their differing implementations result in vastly different performance outcomes. We will implement:\n",
    "\n",
    "1. **Naive Kernel (`src/c/kernels/naive.c`)**  \n",
    "   - A simple, baseline implementation of matrix multiplication to provide a reference point for performance.\n",
    "\n",
    "2. **FP32 Neon Kernel (`src/c/kernels/fp32_neon.c`)**  \n",
    "   - A matrix multiplication optimized for single-precision floating-point operations using Arm Neon SIMD instructions to leverage vectorized computation.\n",
    "\n",
    "3. **INT8 Neon Kernel (`src/c/kernels/int8_neon.c`)**  \n",
    "   - An integer matrix multiplication tailored for 8-bit operations, utilizing Neon SIMD to maximize throughput for lower-precision workloads.\n",
    "\n",
    "By analyzing these implementations, you will gain insight into the performance trade-offs and benefits of hardware-specific optimizations that Arm can offer.\n",
    "\n",
    "#### **Basic Mathematics of Matrix Multiplication**\n",
    "\n",
    "Matrix multiplication is a fundamental operation in linear algebra with wide-ranging applications in computer science, engineering, and especially in machine learning and AI. Given two matrices **A** and **B**, the product **C = A × B** is defined only if the number of columns in **A** matches the number of rows in **B**.\n",
    "\n",
    "Mathematically, if **A** is an *M × K* matrix and **B** is an *K × N* matrix, then their product **C** will be an *M × N* matrix. The element at position *(m, n)* in matrix **C** is computed as a dot product between row *m* of **A** and column *n* of B:\n",
    "\n",
    "$$C_{m,n} = \\sum_{k=1}^{K} A_{m,k} \\times B_{k,n}$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider matrices **A** (2×3) and **B** (3×2):\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23}\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{B} = \\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22} \\\\\n",
    "b_{31} & b_{32}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Their product **C = A × B** (2×2) is:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\begin{bmatrix}\n",
    "c_{11} & c_{12} \\\\\n",
    "c_{21} & c_{22}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "c_{11} &= a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31} \\\\\n",
    "c_{12} &= a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32} \\\\\n",
    "c_{21} &= a_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31} \\\\\n",
    "c_{22} &= a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### **Computational Complexity**\n",
    "\n",
    "The naive matrix multiplication algorithm has a time complexity of **O(n³)**, which becomes computationally expensive for large matrices. Optimizations, such as those leveraging vector instructions, aim to reduce the constant factors and improve cache utilization, thereby enhancing performance without altering the theoretical complexity.\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "### **Implementation of Naive Matrix Multiplication Kernel**\n",
    "\n",
    "The naive implementation of matrix multiplication directly follows the mathematical definition. It uses three nested loops to compute the dot product of rows from matrix **A** and columns from matrix **B** for every element in the resulting matrix **C**. This is the most straightforward and intuitive approach but is computationally expensive due to its **O(n³)** time complexity. We use an example with square matrices in this example, for the sake of simplicity. \n",
    "\n",
    "#### **Code Explanation**\n",
    "\n",
    "1. **Inputs**:\n",
    "   - **A**, **B**: Flattened 2D matrices (stored as 1D arrays in row-major order) to be multiplied.\n",
    "   - **C**: Flattened 2D matrix (1D array, also in row-major) to store the result.\n",
    "   - **N**: Size of the square matrices (number of rows/columns).\n",
    "\n",
    "2. **Procedure**:\n",
    "   - The outer two loops iterate over the rows **m** and columns **n** of the resulting matrix **C**.\n",
    "   - The innermost loop calculates the dot product for each element **C[m, n]** by summing the product of corresponding elements from row **m** of **A** and column **n** of **B**.\n",
    "\n",
    "3. **Performance**:\n",
    "   - This naive implementation is simple, performing one operation at a time but does not leverage advanced optimization techniques, such as blocking, vectorization, or parallelism.\n",
    "\n",
    "#### **Naive Kernel Implementation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir src/c/kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/c/kernels/naive.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/c/kernels/naive.c\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "void matrix_multiply_naive(float* A, float* B, float* C, int K) {\n",
    "    float accumulator;\n",
    "    for (int m = 0; m < K; m++) {\n",
    "        for (int n = 0; n < K; n++) {\n",
    "            accumulator = 0.0f;\n",
    "            for (int k = 0; k < K; k++) {\n",
    "                accumulator += A[m * K + k] * B[k * K + n];\n",
    "            }\n",
    "            C[m * K + n] = accumulator;\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to NEON SIMD Instructions**\n",
    "\n",
    "Neon (Arm Advanced SIMD) is a set of SIMD instructions designed to accelerate data-parallel operations on Arm processors. By leveraging Neon, computations on matrices, such as matrix multiplication, can be vectorized to process multiple data points simultaneously, thereby reducing execution time and increasing throughput.\n",
    "\n",
    "#### **FP32 Matrix Multiplication Using Neon Instructions**\n",
    "\n",
    "1. **Vectorized Processing**:\n",
    "   - In this implementation, the function processes four `float32` elements at a time, utilizing Neon's 128-bit registers (`float32x4_t`).\n",
    "   - The accumulation is performed using fused multiply-add operations, which minimize intermediate memory accesses.\n",
    "\n",
    "2. **Key Neon Instructions Used**:\n",
    "   - **`vld1q_f32`**: Loads four 32-bit floating-point elements into a vector register.\n",
    "   - **`vmlaq_f32`**: Performs a fused multiply-add operation on vectors.\n",
    "     \n",
    "\n",
    "3. **Reduction Step**:\n",
    "   - The vector accumulator is reduced into a scalar using:\n",
    "     - **`vadd_f32`**: Adds low and high parts of the vector.\n",
    "     - **`vpadd_f32`**: Horizontally adds remaining elements for a final scalar result.\n",
    "     - **`vget_lane_f32`**: Extracts a specific element (lane) from a vector, used here to retrieve the final scalar value from the result of `vpadd_f32`.\n",
    "       \n",
    "       \n",
    "4. **Advantages Over Naive FP32 Implementation**:\n",
    "   - **Efficient Memory Access**: The naive implementation loads a single value at a time from memory, which can result in significant memory latency. The Neon implementation processes four elements simultaneously, reducing memory fetch overhead.\n",
    "   - **Reduced Loop Iterations**: By processing multiple elements in parallel, the Neon implementation reduces the number of loop iterations required in the inner loop, significantly improving performance for large matrices.\n",
    "   - **Optimized Accumulation**: Neon's fused multiply-add (`vmlaq_f32`) performs multiplication and addition in a single instruction, minimizing intermediate storage and computation overhead, unlike the naive implementation, which performs these operations sequentially.\n",
    "   - **Hardware Acceleration**: Neon leverages specialized SIMD hardware in the Arm processor, making it much faster than the general-purpose computation used in the naive implementation.\n",
    "\n",
    "5. **Description**:\n",
    "   - The function performs matrix multiplication for `float32` matrices by iterating over the rows and columns of the input matrices, processing four elements at a time in the inner loop. This approach uses Neon to accelerate the computation by leveraging SIMD parallelism.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/c/kernels/fp32_neon.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/c/kernels/fp32_neon.c\n",
    "\n",
    "#include <arm_neon.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "void matmul_fp32_neon(float* A, float* B, float* C, int K) {\n",
    "    for (int m = 0; m < K; m++) {\n",
    "        for (int n = 0; n < K; n++) {\n",
    "            float32x4_t acc = vdupq_n_f32(0.0f); // Accumulator\n",
    "            for (int k = 0; k < K; k += 4) { // Process 4 elements at a time\n",
    "                float32x4_t a_vec = vld1q_f32(&A[m * K + k]); // Load row of A\n",
    "                float32x4_t b_vec = vld1q_f32(&B[k * K + n]); // Load column of B\n",
    "                acc = vmlaq_f32(acc, a_vec, b_vec); // Multiply-accumulate\n",
    "            }\n",
    "            // Reduce acc to a single value and store in C\n",
    "            float32x2_t sum1 = vadd_f32(vget_low_f32(acc), vget_high_f32(acc));\n",
    "            float sum = vget_lane_f32(vpadd_f32(sum1, sum1), 0);\n",
    "            C[m * K + n] = sum;\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to Quantized (INT8) Matrix Multiplication Using Neon SIMD**\n",
    "\n",
    "Quantized matrix multiplication uses low-precision. In this case, 8-bit integer representations to reduce memory bandwidth, power consumption, and computational complexity. Arm processors with Neon (integer 8-bit matrix multiplication) instructions provide significant acceleration by increasing the level of vectorization and benefiting from the efficiency of integer arithmetic.\n",
    "\n",
    "#### **INT8 Matrix Multiplication Using Neon Instructions**\n",
    "\n",
    "1. **Increased Levels of Vectorization**:\n",
    "   - The implementation processes eight `int8_t` elements at a time, leveraging the higher data packing density of 8-bit integers compared to `float32` (four elements at a time). This doubles the level of parallelism compared to the FP32 implementation.\n",
    "\n",
    "2. **Key Neon Instructions Used**:\n",
    "   - **`vld1_s8`**: Loads eight signed 8-bit integers into a vector register.\n",
    "   - **`vmlal_s8`**: Multiplies two vectors of signed 8-bit integers and accumulates the results into 16-bit integers. Accumulation is done in 16-bit to avoid overflow.\n",
    "\n",
    "3. **Reduction Step**:\n",
    "   - **`vaddvq_s16`**: Horizontally sums the elements of a 16-bit integer vector to produce a scalar result.\n",
    "\n",
    "4. **Advantages Over FP32 Implementation**:\n",
    "   - **Higher Vectorization**: Processes eight elements at a time versus four in the FP32 version.\n",
    "   - **Integer Arithmetic**: Integer operations are inherently faster than floating-point operations on most hardware due to simpler hardware requirements.\n",
    "   - **Lower Memory Usage**: `int8_t` data consumes four times less memory than `float32`, leading to reduced cache pressure and better memory bandwidth utilization.\n",
    "   - **Energy Efficiency**: Integer computations typically consume less power, making this approach ideal for energy-constrained environments.\n",
    "\n",
    "5. **Description**:\n",
    "   - The function performs matrix multiplication for quantized `int8` matrices by iterating over the rows and columns of the input matrices. In the inner loop, eight elements are processed simultaneously using Neon SIMD instructions. The 16-bit intermediate results are accumulated, and the final reduction produces a 32-bit scalar result for each element of the output matrix.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/c/kernels/int8_neon.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/c/kernels/int8_neon.c\n",
    "\n",
    "#include <arm_neon.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "void matmul_int8_neon(int8_t* A, int8_t* B, int32_t* C, int N) {\n",
    "    for (int m = 0; m < N; m++) {\n",
    "        for (int n = 0; n < N; n++) {\n",
    "            int16x8_t acc = vdupq_n_s16(0); // Initialize 16-bit accumulator\n",
    "\n",
    "            for (int k = 0; k < N; k += 8) { // Process 8 elements at a time\n",
    "                // Load 8 int8 elements from row of A and column of B\n",
    "                int8x8_t a_vec = vld1_s8(&A[m * N + k]);\n",
    "                int8x8_t b_vec = vld1_s8(&B[k * N + n]);\n",
    "\n",
    "                // Perform element-wise multiplication and accumulate\n",
    "                acc = vmlal_s8(acc, a_vec, b_vec);\n",
    "            }\n",
    "\n",
    "            // Reduce the 16-bit accumulator into a 32-bit scalar\n",
    "            int32_t sum = vaddvq_s16(acc); // Horizontally sum all elements in the vector\n",
    "            C[m * N + n] = sum; // Store the result in C\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Compiling Each Kernel to Inspect Assembly Using GCC**\n",
    "\n",
    "With the kernel implementations ready, we can compile them into assembly code to examine how the compiler leverages Neon hardware-level optimizations to enhance performance. This step provides insights into how SIMD instructions are utilized for accelerating computations, particularly for matrix operations.\n",
    "\n",
    "#### **Steps to Compile Each Kernel**\n",
    "\n",
    "To inspect the generated assembly code for each kernel, use the following GCC command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir bin\n",
    "!gcc -O0 -S -march=armv8-a+simd src/c/kernels/naive.c -o bin/naive.s\n",
    "!gcc -O3 -S -march=armv8-a+simd src/c/kernels/fp32_neon.c -o bin/fp32_neon.s\n",
    "!gcc -O3 -S -march=armv8-a+simd src/c/kernels/int8_neon.c -o bin/int8_neon.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. **Naive Implementation**\n",
    "   - uses scalar instructions exclusively, meaning each operation processes a single pair of data values at a time;\n",
    "   - relies on the following types of Arm assembly instructions:\n",
    "     - **`ldr` (Load Register):** loads a single value from memory into a register\n",
    "     - **`str` (Store Register):** stores a single value from a register into memory\n",
    "     - **`mul` (Multiply):** multiplies two values in registers\n",
    "     - **`add` (Add):** adds two values in registers;\n",
    "   - does not utilize SIMD (Single Instruction Multiple Data) capabilities, which can process multiple data values simultaneously in a single instruction; and\n",
    "   - experiences significant overhead in memory operations due to the frequent use of `ldr` and `str` instructions for each operation, as no batching or parallelism is applied.\n",
    "\n",
    "#### Observations:\n",
    "   - computational units are underutilized because operations are performed serially, one at a time;\n",
    "   - memory bandwidth becomes a bottleneck as frequent loads and stores slow down processing; and\n",
    "   - best suited for small matrices or architectures without support for vectorized operations.\n",
    "\n",
    "#### Example Assembly Output:\n",
    "```assembly\n",
    "ldr\tw1, [sp, 44]   // Load word (32-bit) from stack offset 44 into w1\n",
    "ldr\tw0, [sp, 4]    // Load word (32-bit) from stack offset 4 into w0\n",
    "mul\tw1, w1, w0    // Multiply w1 = w1 * w0\n",
    "ldr\tw0, [sp, 40]   // Load another word from stack offset 40 into w0\n",
    "add\tw0, w1, w0    // Add w0 = w1 + w0\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tadd\tx0, x1, x0\n",
      "\tldr\ts1, [x0]\n",
      "\tldr\tw1, [sp, 32]\n",
      "\tldr\tw0, [sp, 4]\n",
      "\tmul\tw1, w1, w0\n",
      "\tldr\tw0, [sp, 36]\n",
      "\tadd\tw0, w1, w0\n",
      "\tsxtw\tx0, w0\n",
      "\tlsl\tx0, x0, 2\n",
      "\tldr\tx1, [sp, 16]\n",
      "\tadd\tx0, x1, x0\n",
      "\tldr\ts0, [x0]\n",
      "\tfmul\ts0, s1, s0\n",
      "\tldr\ts1, [sp, 44]\n",
      "\tfadd\ts0, s1, s0\n",
      "\tstr\ts0, [sp, 44]\n",
      "\tldr\tw0, [sp, 32]\n"
     ]
    }
   ],
   "source": [
    "!sed -n '34,50p' bin/naive.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. **Neon Vectorization**\n",
    "   - leverages Neon SIMD instructions to perform parallel computations on multiple data values simultaneously:\n",
    "     - **`movi` (Move Immediate):** initializes all elements of a Neon vector register to a specified immediate value (e.g., zero)\n",
    "     - **`fmul` (Floating-Point Multiply):** multiplies corresponding elements in two Neon vector registers\n",
    "     - **`fadd` (Floating-Point Add):** adds corresponding elements in two NEON vector registers\n",
    "     - **`dup` (Duplicate):** copies a scalar value into all elements of a vector register or duplicates one element of a vector across a scalar register\n",
    "     - **`faddp` (Floating-Point Add Pair):** adds adjacent pairs of elements within a vector register, effectively reducing the vector size.\n",
    "   - processes 128-bit registers, enabling parallel computation of up to 4 single-precision floating-point numbers in a single instruction; and\n",
    "   - utilizes optimized memory access patterns to minimize latency and bottlenecks.\n",
    "\n",
    "#### Observations:\n",
    "   - the extracted assembly demonstrates the use of Neon vector registers (e.g., `v0`, `v1`, `v2`) and instructions for efficient parallel floating-point computations; and\n",
    "   - while the exact addresses or register assignments may vary slightly due to compiler differences, the core operations and use of Neon SIMD instructions remain consistent.\n",
    "\n",
    "***NOTE***\n",
    "If the code block below does not show the exact instructions described above, you can open the `bin/fp32_neon.s` file to view the full assembly. Regardless of compiler variations, you should see similar operations (e.g., `movi`, `fmul`, `fadd`, `dup`, `faddp`) utilizing Neon vector registers to perform SIMD optimizations.\n",
    "\n",
    "#### Example Assembly Output:\n",
    "```assembly\n",
    "movi\tv1.4s, 0               // Initialize all elements of v1 to zeros\n",
    "fmul\tv0.4s, v0.4s, v2.4s   // Multiply corresponding elements of v0 and v2, store result in v0\n",
    "fadd\tv1.4s, v1.4s, v0.4s   // Add corresponding elements of v1 and v0, store result in v1\n",
    "dup\td0, v1.d[1]            // Duplicate the second 64-bit element of v1 into scalar register d0\n",
    "fadd\tv0.2s, v0.2s, v1.2s   // Add the lower two elements of v0 and v1, store result in v0.2s\n",
    "faddp\tv0.2s, v0.2s, v0.2s   // Pairwise add elements of v0.2s, reducing it to one scalar\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmovi\tv1.4s, 0\n",
      "\tfmul\tv0.4s, v0.4s, v2.4s\n",
      "\tfadd\tv1.4s, v1.4s, v0.4s\n",
      "\tdup\td0, v1.d[1]\n",
      "\tfadd\tv0.2s, v0.2s, v1.2s\n",
      "\tfaddp\tv0.2s, v0.2s, v0.2s\n"
     ]
    }
   ],
   "source": [
    "!grep -E '\\bv[0-9]+(\\.[a-z0-9]+)?\\b' bin/fp32_neon.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Neon Vectorization (Integer8-based)**\n",
    "\n",
    "This implementation leverages Neon SIMD instructions for efficient integer matrix multiplication. While it does **not** utilize i8mm-specific instructions or other advanced Arm intrinsics, it achieves high performance with quantized data using the following NEON operations:\n",
    "\n",
    "#### Key Neon Instructions Observed:\n",
    "   - **`movi` (Move Immediate):** initializes all elements of a Neon vector register to a specified immediate value, such as zero\n",
    "   - **`smlal` (Signed Multiply-Add Long):** multiplies pairs of 8-bit integers from two vector registers, producing 16-bit results, and accumulates them into a 16-bit vector register\n",
    "   - **`addv` (Add Across Vector):** horizontally sums all elements of a Neon vector register into a single scalar value; and\n",
    "   - **`smov` (Scalar Move):** moves the lowest element from a NEON vector register into a scalar general-purpose register.\n",
    "\n",
    "#### Characteristics:\n",
    "   - processes **8 `int8_t` elements at a time** due to Neon's 128-bit vector registers, allowing for significantly higher throughput compared to `float32` implementations\n",
    "   - accumulates intermediate results in 16-bit registers (`int16`) to avoid overflow during the computation; and\n",
    "   - reduces the final 16-bit vector to a scalar using a horizontal sum (`addv`) followed by moving the scalar value to a general-purpose register (`smov`)\n",
    "\n",
    "#### Observations:\n",
    "   - **Performance Benefits**:\n",
    "     - processes multiple `int8` elements per instruction, maximizing the benefits of vectorization; and\n",
    "     - smaller data types (`int8` vs. `float32`) reduce memory bandwidth requirements and improve efficiency.\n",
    "   - **Precision Limitations**:\n",
    "     - integer arithmetic lacks the precision of floating-point operations, making it best suited for quantized workloads like neural network inference where reduced precision is acceptable.\n",
    "   - **Applications**:\n",
    "     - ideal for embedded systems, mobile devices, and other environments requiring low-power, memory-efficient AI inference.\n",
    "\n",
    "***NOTE***  \n",
    "If the code block below does not show the exact instructions described above, you can open the `bin/int8_neon.s` file to view the assembly. Regardless of compiler or system variations, the assembly will use similar operations (e.g., `movi`, `smlal`, `addv`, `smov`) to optimize the `int8` matrix multiplication using Neon SIMD instructions.\n",
    "\n",
    "#### Example Assembly Output:\n",
    "```assembly\n",
    "movi\tv0.4s, 0               // Initialize v0 to zeros\n",
    "smlal\tv0.8h, v2.8b, v1.8b   // Multiply 8-bit integers from v2 and v1, accumulate into 16-bit vector v0\n",
    "addv\th0, v0.8h             // Horizontally sum all elements in v0.8h into scalar h0\n",
    "smov\tw0, v0.h[0]           // Move the lowest 16-bit value from v0 to scalar register w0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmovi\tv0.4s, 0\n",
      "\tsmlal\tv0.8h, v2.8b, v1.8b\n",
      "\taddv\th0, v0.8h\n",
      "\tsmov\tw0, v0.h[0]\n"
     ]
    }
   ],
   "source": [
    "!grep -E '\\bv[0-9]+(\\.[a-z0-9]+)?\\b' bin/int8_neon.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Benchmarking**\n",
    "\n",
    "Lets now compute the latency of results of these three operators across different matrix sizes to empirically measure their differences. We can set the matrix sizes used in the benchmark by writing them out to **src/c/sizes.c** as seen below. Feel free to adapt the sizes yourself to see how it can effect latency, Bear in mind however large matrix multiplications are compute intensive operations!\n",
    "\n",
    "***Note***\n",
    "This code is tested on Raspberry Pi 5 (Cortex-A76) with matmul size of 1024 can take upto 45s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/c/sizes.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/c/sizes.c\n",
    "\n",
    "int sizes[] = {32, 64, 128, 256, 512};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile and Record Latency With the Naive Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Matrix Multiplication (Size 32): 0.000134 seconds\n",
      "Naive Matrix Multiplication (Size 64): 0.001069 seconds\n",
      "Naive Matrix Multiplication (Size 128): 0.008859 seconds\n",
      "Naive Matrix Multiplication (Size 256): 0.081964 seconds\n",
      "Naive Matrix Multiplication (Size 512): 1.056606 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compile the C code outputting the binary to bin/benchmark_naive\n",
    "!mkdir results\n",
    "!gcc -O0 src/c/benchmark_naive.c -o bin/benchmark_naive -march=armv8-a+simd -lm\n",
    "!./bin/benchmark_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile and Record Latency With the Floating Point SIMD Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 Neon Matrix Multiplication (Size 32): 0.000010 seconds\n",
      "FP32 Neon Matrix Multiplication (Size 64): 0.000073 seconds\n",
      "FP32 Neon Matrix Multiplication (Size 128): 0.000583 seconds\n",
      "FP32 Neon Matrix Multiplication (Size 256): 0.012042 seconds\n",
      "FP32 Neon Matrix Multiplication (Size 512): 0.146004 seconds\n"
     ]
    }
   ],
   "source": [
    "!gcc -O3 -ffast-math src/c/benchmark_fp32_neon.c -o bin/benchmark_fp32_neon -march=armv8-a+simd -lm\n",
    "!./bin/benchmark_fp32_neon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile and Record Latency With the Integer-8 SIMD Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int8 Neon Matrix Multiplication (Size 32): 0.000005 seconds\n",
      "Int8 Neon Matrix Multiplication (Size 64): 0.000035 seconds\n",
      "Int8 Neon Matrix Multiplication (Size 128): 0.000247 seconds\n",
      "Int8 Neon Matrix Multiplication (Size 256): 0.001872 seconds\n",
      "Int8 Neon Matrix Multiplication (Size 512): 0.030810 seconds\n"
     ]
    }
   ],
   "source": [
    "!gcc -O3 -ffast-math src/c/benchmark_int8_neon.c -o bin/benchmark_int8_neon -march=armv8-a+simd -lm\n",
    "!./bin/benchmark_int8_neon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plot the Results** \n",
    "Great, now the operations have been run, lets plot how their latency scales with matrix size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load data from CSV files\n",
    "naive_data = pd.read_csv('results/naive_latency_results.csv')\n",
    "fp32_neon_data = pd.read_csv('results/fp32_neon_latency_results.csv')\n",
    "int8_neon_data = pd.read_csv('results/int8_neon_latency_results.csv')\n",
    "\n",
    "# Extract sizes and times\n",
    "sizes = naive_data['Matrix Size']\n",
    "naive_times = naive_data['Latency (seconds)']\n",
    "fp32_neon_times = fp32_neon_data['Latency (seconds)']\n",
    "int8_neon_times = int8_neon_data['Latency (seconds)']\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the latency for each approach\n",
    "plt.plot(sizes, naive_times, marker='o', linestyle='-', color='r', label='Naive fp32')\n",
    "plt.plot(sizes, fp32_neon_times, marker='o', linestyle='-', color='b', label='Neon SIMD fp32')\n",
    "plt.plot(sizes, int8_neon_times, marker='o', linestyle='-', color='g', label='Neon SIMD int8')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Matrix Size (N x N)')\n",
    "plt.ylabel('Latency (seconds)')\n",
    "plt.title('Latency Comparison: fp32 Naive vs fp32 SIMD vs int8 SIMD Matrix Multiplication')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the graph, the naive approach, which processes each matrix element individually, is the slowest. The SIMD floating-point implementation offers significant speedup, but the SIMD integer implementation is the fastest as the int8 lower bit-width allows a higher degree of parallelism. This highlights the advantage of using lower-precision operations in AI network forward passes for improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **2. Benchmark PyTorch Operations**\n",
    "#### **PyTorch**\n",
    "Having explored how Arm's SIMD capabilities in C can accelerate AI workloads, we now transition to PyTorch which is a versatile, high-level framework that balances flexibility and ease of use. PyTorch streamlines AI model development by abstracting low-level operations, enabling developers to focus on model architecture and experimentation rather than intricate implementation details.\n",
    "\n",
    "Unlike the manual coding required for SIMD operations in C, PyTorch offers built-in support for tensor computations and hardware acceleration. It automatically utilizes processor optimizations, including SIMD and Arm's NEON instructions, to enhance performance without requiring developers to write low-level code.\n",
    "\n",
    "In the next section, we will examine how PyTorch utilizes the Armv8-A's vectorization capability and benchmark Python-based matrix multiplication using both `int8` and `fp32` precision. Building on this foundation, we will implement and optimize the inference of a state-of-the-art small language model, **OpenELM**, showcasing PyTorch's powerful capabilities for handling advanced AI workloads on Armv8.\n",
    "\n",
    "Let's start by checking the build configuration of our installed PyTorch package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch built with:\n",
      "  - GCC 11.2\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) MKL-DNN v3.5.3 (Git Hash 66f0cb9eb66affd2da3bf5f8d897376f04aae6af)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: DEFAULT\n",
      "  - Build settings: BLAS_INFO=open, BUILD_TYPE=Release, COMMIT_SHA=2236df1770800ffea5697b11b0bb0d910b2e59e1, CXX_COMPILER=/opt/rh/gcc-toolset-11/root/usr/bin/c++, CXX_FLAGS=-ffunction-sections -fdata-sections -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_PYTORCH_QNNPACK -DAT_BUILD_ARM_VEC256_WITH_SLEEF -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=open, TORCH_VERSION=2.6.0, USE_CUDA=OFF, USE_CUDNN=OFF, USE_CUSPARSELT=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__config__.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this output, you can see the build configuration for this version of PyTorch. Some key flags indicate how PyTorch is optimized for performance on the your device:\n",
    "\n",
    "- **`USE_OPENMP=ON`**: This library is used to parallelize matrix multiplications across threads, providing speedups on the Raspberry Pi's quad-core processor.\n",
    "- **`BLAS_INFO=ON`** BLAS (Basic Linear Algebra Subprograms) are used to perform efficient linear algebra on Arm utilizing vector processing. \n",
    "- **`USE_NNPACK=ON`**: A low-level library of operators that utilizes vectorized instructions on Arm processors to accelerate operations.\n",
    "- **`USE_MKLDNN=ON`**: While primarily designed for x86 processors, this library also includes vectorized implementations of operators optimized for the AArch64 architecture, making it compatible with devices like the Raspberry Pi 4 and 5.\n",
    "\n",
    "These build configurations show that PyTorch is equipped to take advantage of Arm's vectorization and multi-threading capabilities. \n",
    "\n",
    "Next, let's validate that PyTorch is successfully utilizing these low-level libraries to vectorize its tensor operations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Analyzing PyTorch's Utilization of Arm Vector Processing**\n",
    "\n",
    "To confirm PyTorch's use of Arm's vector processing capabilities, we will investigate its behavior under the hood by writing two simple scripts for matrix multiplication. These scripts replicate what we implemented in C earlier but leverage PyTorch's abstractions. \n",
    "\n",
    "The scripts will perform matrix multiplication using both 32-bit floating-point (`fp32`) and 8-bit integer (`int8`) precisions. PyTorch simplifies these operations by providing built-in functionality that abstracts away low-level details, enabling developers to focus on high-level design while still benefiting from hardware optimizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir src/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/python/fp_matmul.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/python/fp_matmul.py\n",
    "\n",
    "import torch\n",
    "a = torch.randn(1024, 1024, dtype=torch.float32, requires_grad=False)\n",
    "b = torch.randn(1024, 1024, dtype=torch.float32, requires_grad=False)\n",
    "c = torch.mm(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/python/int8_matmul.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/python/int8_matmul.py\n",
    "import torch\n",
    "\n",
    "# Generate random int8 tensors\n",
    "a = torch.randint(-128, 128, size=(1024, 1024), dtype=torch.int8)\n",
    "b = torch.randint(-128, 128, size=(1024, 1024), dtype=torch.int8)\n",
    "c = torch.mm(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using `perf` to Inspect Assembly**\n",
    "\n",
    "The following code cells use the `perf` tool to extract the assembly instructions executed by the Python scripts above. By capturing this assembly, we can analyze whether the low-level libraries that `torch.mm` relies on for matrix multiplication are effectively utilizing Arm's vector processing capabilities, such as Neon instructions.\n",
    "\n",
    "We will write the extracted assembly code to a text file for further analysis. This process allows us to verify if PyTorch's operations are optimized to leverage hardware acceleration on Arm-based systems.\n",
    "\n",
    "**Note**: The following cells may take a few seconds to run. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ perf record: Woken up 1 times to write data ]\n",
      "[ perf record: Captured and wrote 0.044 MB perf.data (64 samples) ]\n"
     ]
    }
   ],
   "source": [
    "!perf record -e instructions:u -c 100000000 -g $(which python) src/python/fp_matmul.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!perf annotate > results/fp_matmul_instructions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Assembly for the Floating-Point matmul**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ perf record: Woken up 1 times to write data ]\n",
      "[ perf record: Captured and wrote 0.045 MB perf.data (66 samples) ]\n"
     ]
    }
   ],
   "source": [
    "!perf record -e instructions:u -c 100000000 -g $(which python) src/python/int8_matmul.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!perf annotate > results/int8_matmul_instructions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Assembly for the int8 matmul**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets have a look inside the instructions used by PyTorch for the floating-point matrix multiply script and print them below. We will search for instructions such as `fmul` to detect their presence and ensure they are using the vector registers for acceleration with Neon SIMD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00 :   10b0ac: add     v0.2s, v0.2s, v1.2s\n",
      "    0.00 :   10b628: add     v0.2s, v0.2s, v1.2s\n",
      "    0.00 :   10b7a4: add     v0.2s, v0.2s, v1.2s\n",
      "    0.00 :   86bc:   mov     v0.d[1], x24\n",
      "    0.00 :   2bb2b8: fmul    v16.4s, v0.4s, v8.s[0]\n",
      "    0.00 :   2bb2bc: fmul    v20.4s, v0.4s, v9.s[0]\n",
      "    0.00 :   2bb2c4: fmul    v24.4s, v0.4s, v10.s[0]\n",
      "    0.00 :   2bb2c8: fmul    v28.4s, v0.4s, v11.s[0]\n",
      "    0.00 :   2bb2d0: fmul    v17.4s, v1.4s, v8.s[0]\n",
      "    0.00 :   2bb2d4: fmul    v21.4s, v1.4s, v9.s[0]\n",
      "    0.00 :   2bb2dc: fmul    v25.4s, v1.4s, v10.s[0]\n",
      "    0.00 :   2bb2e0: fmul    v29.4s, v1.4s, v11.s[0]\n",
      "    0.00 :   2bb2e8: fmul    v18.4s, v2.4s, v8.s[0]\n",
      "    0.00 :   2bb2ec: fmul    v22.4s, v2.4s, v9.s[0]\n",
      "    0.00 :   2bb2f4: fmul    v19.4s, v3.4s, v8.s[0]\n",
      "    0.00 :   2bb2f8: fmul    v23.4s, v3.4s, v9.s[0]\n",
      "    0.00 :   2bb300: fmul    v26.4s, v2.4s, v10.s[0]\n",
      "    0.00 :   2bb304: fmul    v30.4s, v2.4s, v11.s[0]\n",
      "    0.00 :   2bb30c: fmul    v27.4s, v3.4s, v10.s[0]\n",
      "    0.00 :   2bb310: fmul    v31.4s, v3.4s, v11.s[0]\n"
     ]
    }
   ],
   "source": [
    "!grep -m 20 -E '\\bv[0-9]+(\\.[a-z0-9]+)?\\b' results/fp_matmul_instructions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see printed out above some assembly instructions that the Arm processor will run. The structure of each line written out by `perf annotate` has the following format\n",
    "\n",
    "***timestamp :    instruction_address: operation:   destination_register, source_register1, source_register2***\n",
    "\n",
    "The printout should show operations making use of the vector registers. Examples of operations you might see include:   \n",
    "\n",
    "#### **Operation Definitions**\n",
    "- **`fmul`**: Performs a floating-point multiplication operation between two operands.\n",
    "- **`fmla`**: Performs a floating-point fused multiply-add operation, where the result of the multiplication is added to an accumulator in a single instruction.\n",
    "\n",
    "- **Vector Registers**: Source or destination registers starting with a `v` (e.g., `v16.4s`, `v0.4s`, `v8.s[0]`) indicate that Neon SIMD instructions are being used. These registers contain multiple data lanes, enabling the instruction to process multiple elements simultaneously, thereby increasing throughput.\n",
    "  - Example: `fmla v16.4s, v0.4s, v8.s[0]` performs the fused multiply-add operation on four single-precision floating-point elements in parallel (one for each lane of the vector register `v16`)..\n",
    "\n",
    "\n",
    "***Note*** Should no vector registers show up in your instruction list above, please look inside the file results/fp_matmul_instructions.txt to identify any vector register utilization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00 :   451bd68:        dup     v2.16b, w0\n",
      "    0.00 :   451bd78:        mla     v0.16b, v1.16b, v2.16b\n",
      "    0.00 :   451bdb0:        dup     v1.8b, w0\n",
      "    0.00 :   451bdc4:        mla     v0.8b, v2.8b, v1.8b\n",
      "    0.00 :   451c0e4:        movi    v0.4s, #0x0\n",
      "    0.00 :   451c104:        mla     v0.16b, v2.16b, v1.16b\n",
      "    0.00 :   451c110:        addv    b0, v0.16b\n",
      "    0.00 :   451c11c:        umov    w12, v0.b[0]\n",
      "    0.00 :   451c15c:        mul     v0.8b, v0.8b, v1.8b\n",
      "    0.00 :   451c160:        addv    b0, v0.8b\n",
      "    0.00 :   451c164:        umov    w12, v0.b[0]\n",
      "    0.00 :   451c3d0:        dup     v2.16b, w0\n",
      "    0.00 :   451c3e0:        mla     v0.16b, v1.16b, v2.16b\n",
      "    0.00 :   451c414:        dup     v1.8b, w0\n",
      "    0.00 :   451c428:        mla     v0.8b, v2.8b, v1.8b\n",
      "    0.00 :   451c740:        movi    v3.4s, #0x0\n",
      "    0.00 :   451c750:        mov     v22.16b, v3.16b\n",
      "    0.00 :   451c754:        mov     v21.16b, v3.16b\n",
      "    0.00 :   451c758:        mov     v9.16b, v3.16b\n",
      "    0.00 :   451c76c:        ld4     {v4.16b-v7.16b}, [x9], #64\n"
     ]
    }
   ],
   "source": [
    "!grep -m 50 -E '\\bv[0-9]+(\\.[a-z0-9]+)?\\b' results/int8_matmul_instructions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of instructions you might see in the output above are: \n",
    "- **`dup`**: Duplicates the value of a scalar register (e.g., `w0`) into all lanes of a SIMD vector register (e.g., `v2.16b`). This allows the same value to be broadcast across multiple lanes for parallel processing.\n",
    "- **`mla`**: Performs a fused multiply-accumulate operation. It multiplies corresponding elements from two SIMD vector registers and adds the results to the accumulator register. This operation is performed on all lanes in parallel.\n",
    "- **`addv`**: Adds all elements in a SIMD vector register and stores the resulting sum in a scalar register. This is typically used for reduction operations to aggregate data from multiple lanes during accumulation of dot product results.\n",
    "- **`umov`**: Extracts a specific lane from a SIMD vector register and moves it to a scalar register. This is useful for accessing individual elements after SIMD processing (e.g. accessing the sum of a dot product).\n",
    "\n",
    "The presence of these instructions shown above indicate that torch is utilizing Arm's SIMD capabilities. \n",
    "\n",
    "***Note*** Should no vector registers show up in the instruction list above, please look inside the file results/fp_matmul_instructions.txt to identify any vector register utilization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Benchmarking PyTorch Linear Layer**\n",
    "Great. Now you have verified that PytTorch is utilizing the vector processing capability of Arm by default, we can benchmark it's operations. Specifically, let's benchmark the floating-point and integer precision operations. Just like our above kernel examples written in C. We should see a measurable latency reduction when processing in int8 precision vs fp32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a Latency Measuring Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import torch \n",
    "import numpy as np\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "def benchmark(inputs, func, num_runs=10): \n",
    "    times = []\n",
    "    for _ in range(3): # warm up \n",
    "        func(inputs)\n",
    "\n",
    "    for _ in range(num_runs):  # timing runs\n",
    "        st = time.time()\n",
    "        func(inputs)\n",
    "        times.append(time.time() - st)\n",
    "\n",
    "    return np.mean(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once Again Specify the Size of Matrices to Evaluate**\n",
    "\n",
    "Feel free to adjust the matrix sizes to identify how much computation large matrix multiply's require!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "sizes = [32, 64, 128, 256, 512, 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Record Latencies for a Full Precision Linear Layer**  \n",
    "The linear layer is a fundamental building block of generative AI models like transformers, performing dense matrix multiplications at its core. Given input $ X \\in \\mathbb{R}^{m \\times n} $ and weights $ W \\in \\mathbb{R}^{n \\times p} $, the output $ Y \\in \\mathbb{R}^{m \\times p} $ is computed as $ Y = XW + b $, where $ b $ is an optional bias term. Measuring the latency of these FP32 matrix multiplications provides a baseline for assessing performance and comparing with optimized or quantized implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "float_times = []\n",
    "for size in sizes:\n",
    "    # floating-point measurements \n",
    "    x = torch.randn(size, size, dtype=torch.float32, requires_grad=False)\n",
    "    linear = nn.Linear(size, size, bias=False)\n",
    "    float_times.append(benchmark(x, linear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Record Latencies for an INT8 Quantized Linear Layer**  \n",
    "In a quantized linear layer, the computation $ Y = XW + b $ is performed using INT8 precision for the operands $ X $ (input) and $ W $ (weights), while optionally adding a bias $ b $ in a higher precision (e.g., INT32 or FP32) to preserve accuracy. Quantization maps the original floating-point values to 8-bit integers using a scale $ S $ and zero-point $ Z $, such that $ x_\\text{quant} = \\text{round}(x / S) + Z $. This allows efficient matrix multiplications in reduced precision while maintaining a close approximation of the original computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.quantized as nnq\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "int8_times = []\n",
    "for size in sizes:\n",
    "    # floating point measurements \n",
    "    x = torch.randn(size, size, dtype=torch.float32, requires_grad=False)\n",
    "    x_quant = torch.quantize_per_tensor(x, scale=x.abs().max()/127, zero_point=0, dtype=torch.qint8)\n",
    "    qlinear = nnq.Linear(size, size, dtype=torch.qint8)\n",
    "    qlinear.set_weight_bias(x_quant, None)\n",
    "    int8_times.append(benchmark(x_quant, qlinear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Scaling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the latency for each approach\n",
    "plt.plot(sizes, float_times, marker='o', linestyle='-', color='r', label='float precision')\n",
    "plt.plot(sizes, int8_times, marker='o', linestyle='-', color='b', label='int8 precision')\n",
    "# Adding labels and title\n",
    "plt.xlabel('Matrix Size (N x N)')\n",
    "plt.ylabel('Latency (seconds)')\n",
    "plt.title('Latency Comparison: Floating-point vs Integer Linear Layers')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see that for small matrices, there is little difference in latency for processing floating-point and integer linear layers. As the matrix size increases, the integer linear layer latency should increase slower than floating-point. Therefore if we can quantize linear layers, used by generative AI models from floating-point to integer values without losing too much accuracy, we can see a significant latency improvement (not to mention reduced memory consumption)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **3. Run Inference on a Language Model**\n",
    "\n",
    "### How to Download and Use **OpenELM** from Hugging Face\n",
    "\n",
    "OpenELM is a recently released, small open-source language model by Apple, designed for efficient local deployment [(OpenELM Paper)](https://arxiv.org/abs/2404.14619). It comes in compact sizes: 270M, 450M, 1.1B, and 3B, making it ideal for embedded applications. In this lab, we will use OpenELM-270M and focus on optimizing its inference speed on Arm devices.\n",
    "\n",
    "1. **Log In or Sign Up**  \n",
    "   If you don’t already have a Hugging Face account, create one. Otherwise, log in to your existing account.\n",
    "\n",
    "2. **Visit the Model Page**  \n",
    "   Navigate to the [apple/OpenELM](https://huggingface.co/apple/OpenELM) page on Hugging Face.\n",
    "\n",
    "3. **Request Access**  \n",
    "   On the model's page, click the **\"Access repository\"** button if present. You’ll be prompted to review and agree to the terms of use.\n",
    "\n",
    "4. **Visit the Lamma Model Page**\n",
    "   Navigate to the [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) page on Hugging Face. You need to do this, as the OpenELM model uses the Lamma Tokenizer. \n",
    "\n",
    "5. **Request Access**  \n",
    "   On the model's page, click the **\"Access repository\"** button. You’ll be prompted to review and agree to the terms of use.\n",
    "\n",
    "6. **Wait for Approval**  \n",
    "   After agreeing to the terms, access will be granted. This may take a few hours where you should receive a notificatino via email.\n",
    "\n",
    "7. **Login via the Command Line**\n",
    "\n",
    "To download the model, you may need to authenticate your Hugging Face account on your local machine. Run the following command in your terminal and follow the prompts to log in (this will involve creating an access token for your local machine, a 'Read' token is sufficient):\n",
    "\n",
    "   ```bash\n",
    "   huggingface-cli login\n",
    "   ```\n",
    "\n",
    "After following the prompted authentication steps, you can download and use the model with the following script.\n",
    "\n",
    "**NOTE** This script may take up to half an hour to run the first time it is executed. This is because the Hugging Face package needs to download the model weights. After the initial execution, reloading the model should be faster (45s on Raspberry Pi 5) since Hugging Face caches the weights locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A new version of the following files was downloaded from https://huggingface.co/apple/OpenELM-270M-Instruct:\n",
      "- configuration_openelm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/apple/OpenELM-270M-Instruct:\n",
      "- modeling_openelm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenELMForCausalLM(\n",
       "  (transformer): OpenELMModel(\n",
       "    (token_embeddings): Embedding(32000, 1280)\n",
       "    (layers): ModuleList(\n",
       "      (0): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (proj_2): Linear(in_features=768, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (1): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=2048, bias=False)\n",
       "          (proj_2): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (2): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=2560, bias=False)\n",
       "          (proj_2): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (3): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=3072, bias=False)\n",
       "          (proj_2): Linear(in_features=1536, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (4): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=3584, bias=False)\n",
       "          (proj_2): Linear(in_features=1792, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (5): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (proj_2): Linear(in_features=2048, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (6): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=5120, bias=False)\n",
       "          (proj_2): Linear(in_features=2560, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (7): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=5632, bias=False)\n",
       "          (proj_2): Linear(in_features=2816, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (8): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=6144, bias=False)\n",
       "          (proj_2): Linear(in_features=3072, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (9): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=6656, bias=False)\n",
       "          (proj_2): Linear(in_features=3328, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (10): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=7168, bias=False)\n",
       "          (proj_2): Linear(in_features=3584, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (11): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=7680, bias=False)\n",
       "          (proj_2): Linear(in_features=3840, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (12): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=8704, bias=False)\n",
       "          (proj_2): Linear(in_features=4352, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (13): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=9216, bias=False)\n",
       "          (proj_2): Linear(in_features=4608, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (14): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=9728, bias=False)\n",
       "          (proj_2): Linear(in_features=4864, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (15): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=10240, bias=False)\n",
       "          (proj_2): Linear(in_features=5120, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-270M-Instruct\", trust_remote_code=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's Run a Prompt Through the Instruction-Tuned Model and See What It Generates**\n",
    "\n",
    "OpenELM is a small, instruction-finetuned language model trained to understand and generate outputs tailored to specific instructions or tasks. Unlike its base version, which functions as an autocomplete engine, the instruction-tuned model is optimized to follow detailed prompts and generate contextually relevant completions or answers.\n",
    "\n",
    "In this lab, we will explore its capabilities by providing it with a prompt and observing how it completes the instruction. For demonstration purposes, we’ll set a maximum sequence length of 30 tokens to manage computational costs and observe its output for the prompt: \n",
    "\n",
    "***\"Arm is a company that designs\"***.\n",
    "\n",
    "**Note:** Generative models, are stochasic, and hence your output maybe slightly different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm is a company that designs and manufactures microcontrollers, which are tiny electronic devices that can perform specific functions. The company has been\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Arm is a company that designs\" \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=30)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's Measure the Static Memory Consumption of the Model in PyTorch Format.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Model Size: 1035.8 MB\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for file in filenames:\n",
    "            file_path = os.path.join(dirpath, file)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "    return total_size\n",
    "\n",
    "model.save_pretrained(\"checkpoints/OpenELM-hf\")\n",
    "pytorch_model_size = get_directory_size(\"checkpoints/OpenELM-hf\") / (1024 ** 2)\n",
    "print(f\"PyTorch Model Size: {pytorch_model_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a significant amount of memory consumption, 1GB for the 270M model! Many small embedded devices may struggle to handle this. Therefore, it's crucial to explore strategies for reducing the model's memory footprint, alongside improving inference speed. Doing so will expand the range of devices capable of running the model and, since memory access is energy-intensive, it will also reduce energy consumption and extend battery life. Next however, lets look at the latency bottlenecks and dynamic memory usage during inference with `torch.profiler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                      model_inference        15.92%      16.621ms       100.00%     104.372ms     104.372ms           0 b    -216.54 Mb             1  \n",
      "                                         aten::linear         0.14%     147.371us        62.37%      65.094ms       1.001ms      52.28 Mb           0 b            65  \n",
      "                                         aten::matmul         0.51%     533.831us        61.90%      64.610ms     994.006us      52.28 Mb           0 b            65  \n",
      "                                             aten::mm        61.09%      63.765ms        61.11%      63.779ms     981.211us      52.28 Mb      52.28 Mb            65  \n",
      "                                            aten::mul         4.07%       4.243ms         4.08%       4.257ms      20.081us      74.29 Mb      74.29 Mb           212  \n",
      "                   aten::scaled_dot_product_attention         0.12%     122.288us         3.17%       3.310ms     206.868us       4.43 Mb     -70.88 Kb            16  \n",
      "    aten::_scaled_dot_product_flash_attention_for_cpu         2.81%       2.930ms         3.05%       3.188ms     199.225us       4.50 Mb      -2.16 Mb            16  \n",
      "                                           aten::mean         0.31%     328.281us         2.57%       2.683ms      41.275us      97.88 Kb      97.84 Kb            65  \n",
      "                                           aten::silu         2.53%       2.637ms         2.53%       2.637ms     164.782us      12.80 Mb      12.80 Mb            16  \n",
      "                                          aten::copy_         1.99%       2.081ms         1.99%       2.081ms      12.239us           0 b           0 b           170  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 104.372ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "prompt = \"Arm is a company that designs and develops microprocessors and other microchips. It is also known for its Arm Cortex processor and the Arm Mali GPU. Arm has been a leader in the mobile computing industry since the early 2000s, and is now a major player in the AI and \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Profile the model\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU],\n",
    "    record_shapes=True,\n",
    "    with_stack=True,\n",
    "    profile_memory=True,\n",
    ") as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "# Print a summary\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look something like this for OpenELM-270M\n",
    "\n",
    "<div style=\"overflow-x: auto; font-family: monospace;\">\n",
    "<pre>\n",
    "| **Name**                                      | Self CPU % | Self CPU   | CPU total % | CPU total   | CPU time avg  | CPU Mem      | Self CPU Mem | # of Calls |\n",
    "|-----------------------------------------------|------------|------------|-------------|-------------|---------------|--------------|--------------|------------|\n",
    "| model_inference                               | 6.98%      | 141.663ms  | 100.00%     | 2.029s      | 2.029s        | 16.54 Mb     | -204.71 Mb   | 1          |\n",
    "| aten::matmul                                  | 0.74%      | 14.967ms   | 72.95%      | 1.480s      | 15.262ms      | 61.69 Mb     | 0 b          | 97         |\n",
    "| aten::linear                                  | 0.16%      | 3.303ms    | 66.95%      | 1.359s      | 20.901ms      | 52.28 Mb     | 0 b          | 65         |\n",
    "| aten::mm                                      | 66.16%     | 1.342s     | 66.16%      | 1.342s      | 20.654ms      | 52.28 Mb     | 52.28 Mb     | 65         |\n",
    "| aten::scaled_dot_product_attention            | 0.24%      | 4.773ms    | 8.46%       | 171.761ms   | 10.735ms      | 4.43 Mb      | -4.98 Mb     | 16         |\n",
    "| aten::_scaled_dot_product_attention_math      | 0.08%      | 1.567ms    | 8.23%       | 166.988ms   | 10.437ms      | 9.41 Mb      | -13.84 Mb    | 16         |\n",
    "| aten::bmm                                     | 4.62%      | 93.766ms   | 5.68%       | 115.326ms   | 3.604ms       | 9.41 Mb      | 4.54 Mb      | 32         |\n",
    "| aten::embedding                               | 1.21%      | 24.580ms   | 3.59%       | 72.850ms    | 72.850ms      | 360.00 Kb    | 0 b          | 1          |\n",
    "| aten::silu                                    | 3.18%      | 64.437ms   | 3.18%       | 64.437ms    | 4.027ms       | 12.80 Mb     | 12.80 Mb     | 16         |\n",
    "| aten::index_select                            | 1.70%      | 34.598ms   | 2.24%       | 45.386ms    | 45.386ms      | 360.00 Kb    | 360.00 Kb    | 1          |\n",
    "\n",
    "Self CPU time total: 2.029s\n",
    "</pre>\n",
    "</div>\n",
    "\n",
    "**Self CPU time total:** 2.029s\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways from Profiling\n",
    "\n",
    "- **Matrix Multiplications (aten::mm)**: The dominant computational operation, accounting for roughtly 66% of the total CPU computation time. Dense layers (e.g., `aten::linear`) rely heavily on these operations.\n",
    "- **Scaled Dot Product Attention**: The attention mechanism contributes to around 8.46% of the total computation time and uses relatively moderate memory at 4.43 MB. This may increase for increased context lengths, due to the quadratic complexity of the attention mechanism.\n",
    "- **Memory Usage**: Matrix multiplications (`aten::mm` or `aten::mul`) are the most memory-intensive operation, allocating roughly 52 MB. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Optimization Focus**\n",
    "\n",
    "**Matrix multiplications in Dense Layers Are Among the Most Critical Operations to Optimize.**  \n",
    "These operations dominate both computation and memory usage, making them a primary target for performance improvements. Optimization techniques such as vectorization and quantization, discussed in earlier sections, can significantly enhance efficiency and reduce memory usage.\n",
    "\n",
    "In this section, we will begin by lowering the precision of the feedforward linear layers (heavily reliant on matrix multiplications) to the `int8` datatype. This process, known as **quantization**, can greatly improve performance. Specifically, we will explore a technique called **Integer Per-Tensor Symmetric Weight-Only Quantization**. A bit of a mouthful, but let’s break it down how it works. \n",
    "\n",
    "#### **Integer Per-Tensor Symmetric Weight-Only Quantization**\n",
    "\n",
    "Integer symmetric weight-only quantization reduces a neural network's weight precision from floating-point to integer values, while keeping activations in floating-point format during inference, hence why it is called weight-only. Although in this format, the underlying matrix multiplication still occurs in floating-point, for example via `torch.mm(W.dequantize(), x)`, weight-only quantization can decrease the model's static memory consumption by up to 4x. Additionally in cases where inference speed is limited by memory bandwidth, reducing the weight precision can also lower memory read latencies, further speeding up inference. Now lets go through some of the maths:\n",
    "\n",
    "Given that the equation of the linear layers is outlined below, where y is the output activation, $X^{fp}$ is the input activation, $W^{fp}$ the floating-point weight matrix and $B^{fp}$ the bias, where the superscript $fp$ denotes that the input activation, weight, and bias are in floating-point\n",
    "\n",
    "$y = X^{fp}W^{fp} + B^{fp}$\n",
    "\n",
    "To apply Symmetric Per-Tensor weight-only quantization, the floating-point weights $W^{fp}$ are quantized to integer values $ W^{q} $ using a single scale factor $ S_w $ which is a floating-point value. This scalar maps the floating-point range of the weight, to integer range e.g. (-128, 127) for `int8`. It can be computed based on the maximum absolute weight value: where |W| is the absolute weight matrix and $b$ is the number of bits in the integer value, for `int8` it is 8. (Note: don't confuse the bias $B^{fp}$ with the number of bits $b$)\n",
    "\n",
    "$\n",
    "S_w = \\frac{\\max(|W^{fp}|)}{2^{b-1} - 1}\n",
    "$\n",
    "\n",
    "\n",
    "Given that this scale $S_w$ value maps the floating-point weight range to the integer weight range. The quantization and dequantization processes are given by:\n",
    "\n",
    "- **Quantization**:\n",
    "  $\n",
    "  W^{q} = Q_w(W^{fp}) = \\text{round}\\left( \\frac{W^{fp}}{S_w} \\right)\n",
    "  $\n",
    "\n",
    "- **Dequantization**:\n",
    "  $\n",
    "  W^{fp} \\approx \\hat{W}^{fp} = DQ(W^{q}, S_w) = W^q \\cdot S_w\n",
    "  $\n",
    "\n",
    "\n",
    "During inference, both the weights and activations must share the same precision for the hardware's multipliers to perform matrix multiplication. When weights are quantized to `int8` $ W^q $ but activations remain in floating-point $X^{fp}$, a mismatch occurs. To resolve this, the quantized weights must be dequantized back into floating-point values before computation.\n",
    "\n",
    "$\n",
    "y = X^{fp} \\hat{W}^{fp}  + B^{fp} \\approx X^{fp} (W^q \\cdot S_w) + B^{fp}\n",
    "$\n",
    "\n",
    "\n",
    "Lets now collect an intermediate weight matrix and activation tensor from one of the feed forward layers in the OpenELM model so we can practice this quantization technique. We can do this collection using a functionality of PyTorch called forward hooks. You can set the variable `layer_idx` in the code below to select a given transformer block from the OpenELM model from which you want to extract the weight matrix and activation tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The collected weight matrix has shape 1280x4352 (output_channels x input_channels)\n",
      "The collected input activation tensor has shape 1x72x4352 (batch_size x sequence_length x input_channels)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Initialize dictionary to store activations\n",
    "activations = {}\n",
    "\n",
    "# Define a hook function to capture input\n",
    "def get_activation_input(name):\n",
    "    def hook(model, input, output):\n",
    "        # 'input' is a tuple; we take the first element for the input tensor\n",
    "        activations[name] = input[0]\n",
    "    return hook\n",
    "\n",
    "# Select the transformer block and register the hook\n",
    "layer_idx = 12  # Select the transformer block of your choosing\n",
    "layer = model.transformer.layers[layer_idx].ffn.proj_2\n",
    "hook_handle = layer.register_forward_hook(get_activation_input('proj_2'))\n",
    "\n",
    "# Run the model forward pass\n",
    "with torch.no_grad():\n",
    "    model(**inputs)\n",
    "\n",
    "# Extract the weight matrix and the input activation tensor from the hook\n",
    "W_fp = layer.weight.data.clone()\n",
    "X_fp = activations['proj_2']\n",
    "hook_handle.remove()\n",
    "\n",
    "print(f\"The collected weight matrix has shape {W_fp.shape[0]}x{W_fp.shape[1]} (output_channels x input_channels)\")\n",
    "print(f\"The collected input activation tensor has shape {X_fp.shape[0]}x{X_fp.shape[1]}x{X_fp.shape[2]} (batch_size x sequence_length x input_channels)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have collected a weight matrix and activation tensor from the inside of the OpenELM feedforward layers (we omit the bias as OpenELM feedforward layers do not uses baises). We can now use these to test the accuracy of the quantization technique. Lets start by implementing the per-tensor quantization technque described above. We will omit the bias, as in the case of the OpenELM model, its feedforward layers do not use biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-compute the output of the linear layer before quantization to compare against our quantized version. \n",
    "y = torch.matmul(X_fp, W_fp.t()) \n",
    "\n",
    "# Quantize the layer weight 'W'\n",
    "b = 8  # Bit-width for quantization (int8 quantization)\n",
    "S_w = W_fp.abs().max() / (2**(b-1) - 1)  # Scale Factor Quantization\n",
    "W_q = torch.round(W_fp / S_w).clamp(-2**(b-1), 2**(b-1) - 1)  # Quantizing the weights (clamp is used to prevent overflow)\n",
    "\n",
    "# Weight-only quantized linear layer\n",
    "y_hat = torch.matmul(X_fp, (W_q * S_w).t()) # Notice the Dequantzation step here (Q * S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the quantization error to see if we have correctly approximated the full precision linear layer. This can be defined as the absolute difference between the full precision linear layer and our quantized version. Using our notation, where $y$ is the full precision output, $\\hat{y}$ is the quantized output, $Q_w$ is the quantized tensor, and $S_w$ is the scale factor, the quantization error can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{Quantization Error} = \\left| y - \\hat{y} \\right| = \\left| y - X^{fp} \\cdot (W^q \\cdot S_w) \\right|\n",
    "$$\n",
    "\n",
    "Additionally, the relative quantization error as a percentage can be calculated to understand the error in relation to the magnitude of the full precision output:\n",
    "\n",
    "$$\n",
    "\\text{Relative Quantization Error (\\%)} = \\left( \\frac{\\left| y - \\hat{y} \\right|}{\\left| y \\right|} \\right) \\times 100\n",
    "$$\n",
    "\n",
    "This relative error provides a normalized measure of the quantization error, making it easier to compare across different scales of output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max relative quantization error: 319890.91 (%)\n",
      "Min relative quantization error: 0.00 (%)\n",
      "Mean relative quantization error: 37.32 (%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate quantization error. This is the difference between the full precision linear layer and our quantized version. \n",
    "residuals = (y - y_hat).abs()\n",
    "residuals_rel = (residuals / y.abs()) * 100\n",
    "print(f\"Max relative quantization error: {residuals_rel.max():.2f} (%)\")\n",
    "print(f\"Min relative quantization error: {residuals_rel.min():.2f} (%)\")\n",
    "print(f\"Mean relative quantization error: {residuals_rel.mean():.2f} (%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the mean, min and max relative quantization error. The max relative quantization error may be huge. This is caused by outliers in the activation and/or weight matrix that can skew the quantization error. There are a number of techniques to mitigate this issue. We will not cover them here but you can learn more about them in the [LLM.int8() paper](https://arxiv.org/abs/2208.07339). \n",
    "\n",
    "This is a good start but the mean quantization error is still relativly high which may cause accuracy degredation. We can do better. To do so lets try and understand what the distribution of the data in the weight matrix looks like, this will give an indication as to why the quantization error might be so high. Below we will plot a boxplot of the 5 largest and 5 smallest output channels in your chosen layer (set by the layer_idx variable). This essentially plots the weight distribution of the largest and smallest rows in the weight matrix $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4352, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the L2 norm of each channel along axis 1\n",
    "channel_magnitudes = np.linalg.norm(W_q.clone().numpy(), axis=1)\n",
    "\n",
    "num_channels_to_select = 5\n",
    "largest_channels_indices = np.argsort(channel_magnitudes)[-num_channels_to_select:]\n",
    "smallest_channels_indices = np.argsort(channel_magnitudes)[:num_channels_to_select]\n",
    "\n",
    "# Select the largest and smallest channels along axis 1\n",
    "largest_channels = W_q.numpy()[largest_channels_indices, :]\n",
    "smallest_channels = W_q.numpy()[smallest_channels_indices, :]\n",
    "\n",
    "# Combine the data for boxplot\n",
    "combined_data = np.concatenate((largest_channels, smallest_channels), axis=0)\n",
    "labels = ['Largest'] * num_channels_to_select + ['Smallest'] * num_channels_to_select\n",
    "\n",
    "combined_data = combined_data.T\n",
    "print(combined_data.shape)\n",
    "# Plot boxplot for the selected channels\n",
    "def plot_combined_boxplot(combined_data, labels):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=combined_data, palette=\"Set2\")\n",
    "    plt.xticks(ticks=np.arange(len(labels)), labels=labels * (combined_data.shape[1] // len(labels)))\n",
    "    plt.title(f'BoxPlot Distributions of Largest and Smallest Channels in Lamma3.2-1B FeedForward Layer {layer_idx}')\n",
    "    plt.xlabel('Channel Type')\n",
    "    plt.ylabel('Weight Value')\n",
    "    plt.show()\n",
    "\n",
    "plot_combined_boxplot(combined_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Per-Tensor Symmetric Weight-Only Quantization\n",
    "\n",
    "Looking at this plot, you can see that the larger channels exhibit broad distributions with large outliers, whereas the smaller channels display tight distributions centered around zero. If we were to apply per-tensor quantization, which uses a single scale value $ S_w $ computed as `W.abs().max() / (2**(b-1) - 1)`, the presence of outliers in the largest channel would disproportionately inflate the numerator. Since the denominator is fixed by the quantization bit-width $ b $, this results in an overly large scale factor. Consequently, the quantization process would lead to substantial rounding errors, thereby degrading the overall accuracy of the model.\n",
    "\n",
    "To overcome this, we can use a technique called **Symmetric Per-Channel Weight-Only Quantization** where we increase the granularity of quantization from **Per-Tensor** to **Per-Channel**. Instead of using only a single scale factor for the entire tensor, we compute a separate scale factor for each output channel of the weight matrix (row of the matrix $W_fp$), thereby quantizing each seperately. This allows us to more accurately quantize the weights in each channel, reducing the quantization error, and preserving the accuracy of the model.\n",
    "\n",
    "The process for doing this is as follows:\n",
    "\n",
    "#### 1. Channel-Wise Scale Factor\n",
    "For each channel $i$ of $\\mathbf{W}$ (i.e., row $\\mathbf{W}_i$), compute a channel-specific scale factor:\n",
    "\n",
    "$\n",
    "S_{w_i} = \\frac{\\max(|\\mathbf{W}^{fp}_i|)}{2^{b-1} - 1}.\n",
    "$\n",
    "\n",
    "#### 2. Channel-Wise Quantization and Dequantization\n",
    "1. **Quantization**: Convert the weights of each output channel indexed by $i$ to integer values: \n",
    "\n",
    "   $\n",
    "   W^q_{i} = \\text{round}\\left(\\frac{\\mathbf{W}^{fp}_i}{S_{w_i}}\\right)\n",
    "   $\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. **Dequantization**: Recover the approximate floating-point weights of each channel $i$:\n",
    "\n",
    "   $\n",
    "   \\hat{\\mathbf{W}}_i = Q_{w_i} \\cdot S_{w_i}.\n",
    "   $\n",
    "\n",
    "The final quantized weight matrix $\\hat{\\mathbf{W}}$ is reconstructed by concatenating the channel-wise dequantized weights. (This can be vectorized by torch's broadcasting feature)\n",
    "\n",
    "#### 3. Inference\n",
    "During inference, the computation of $\\mathbf{y}$ remains similar but uses the per-channel dequantized weights:\n",
    "\n",
    "$\n",
    "\\mathbf{y} = X^{fp} \\hat{W}^{fp} \\approx X^{fp} \\left(\\sum_{i=1}^m W^q_{i} \\cdot S_{w_i}\\right).\n",
    "$\n",
    "\n",
    "This approach ensures that both small and large channels are appropriately quantized, reducing overall quantization error, particularly when different output channels have very different distrubiton ranges. Lets implement this below. \n",
    "\n",
    "## Exercise 1 \n",
    "Your task is to complete the missing line in the code cell below that computs the per-channel scale factor. You will need to use the above explanation to per-channel quantization scale computation. \n",
    "\n",
    "**NOTE**\n",
    "if you are struggling there is an example solution at the end of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'S_per_channel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m  \u001b[38;5;66;03m# Bit-width for quantization\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ------ Complete the missing line here ------\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# you need to compute a different scale factor for each channel in the weight matrix. \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# this will result in a scale 'S' vector with the same number of output channels as the weight matrix. \u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# S_per_channel = ... \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m Q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(W_fp \u001b[38;5;241m/\u001b[39m \u001b[43mS_per_channel\u001b[49m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(b\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(b\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Q <- quantize(W)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Weight-only quantized linear layer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m y_hat_per_channel \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(X_fp, (Q \u001b[38;5;241m*\u001b[39m S_per_channel)\u001b[38;5;241m.\u001b[39mt()) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'S_per_channel' is not defined"
     ]
    }
   ],
   "source": [
    "# Re-compute the output of the linear layer before quantization. \n",
    "y = torch.matmul(X_fp, W_fp.t()) \n",
    "\n",
    "# Quantize the layer weight 'W'\n",
    "b = 8  # Bit-width for quantization\n",
    "\n",
    "# ------ Complete the missing line here ------\n",
    "# you need to compute a different scale factor for each channel in the weight matrix. \n",
    "# this will result in a scale 'S' vector with the same number of output channels as the weight matrix. \n",
    "# S_per_channel = ... \n",
    "# ---------------------------------------------\n",
    "Q = torch.round(W_fp / S_per_channel).clamp(-2**(b-1), 2**(b-1) - 1)  # Q <- quantize(W)\n",
    "\n",
    "# Weight-only quantized linear layer\n",
    "y_hat_per_channel = torch.matmul(X_fp, (Q * S_per_channel).t()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can now test your implementation by running the test below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max relative per-channel quantization error: 40039.36 (%), per-tensor quantization error: 319890.91 (%)\n",
      "Min relative per-channel quantization error: 0.00 (%), per-tensor rel quantization error: 0.00 (%)\n",
      "Mean relative per-channel quantization error: 7.64 (%), per-tensor mean rel quantization error: 37.32 (%)\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='color: green; font-size: 20px;'>Success! Per-Channel quantization error: 7.64% and Per-Tensor quantization error 37.32%</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Calculate quantization error\n",
    "residuals_per_channel = (y - y_hat_per_channel).abs()\n",
    "residuals_rel_per_channel = (residuals_per_channel / y.abs()) * 100\n",
    "print(f\"Max relative per-channel quantization error: {residuals_rel_per_channel.max():.2f} (%), per-tensor quantization error: {residuals_rel.max():.2f} (%)\")\n",
    "print(f\"Min relative per-channel quantization error: {residuals_rel_per_channel.min():.2f} (%), per-tensor rel quantization error: {residuals_rel.min():.2f} (%)\")\n",
    "print(f\"Mean relative per-channel quantization error: {residuals_rel_per_channel.mean():.2f} (%), per-tensor mean rel quantization error: {residuals_rel.mean():.2f} (%)\")\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "if residuals_rel_per_channel.mean() < residuals_rel.mean(): \n",
    "    display(HTML(f\"<div style='color: green; font-size: 20px;'>Success! Per-Channel quantization error: {residuals_rel_per_channel.mean():.2f}% and Per-Tensor quantization error {residuals_rel.mean():.2f}%</div>\"))\n",
    "else: \n",
    "    display(HTML(f\"<div style='color: red; font-size: 20px;'>Failed. Mean relative quantization error: {residuals_rel_per_channel.mean():.2f}%. Please try again.</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Great! You should see that the per-channel weight-only quantization technique has a significantly lower quantization error than per-tensor quantization. Let's now use this approach to quantize OpenELM! Specifically we will apply quantization to the feedforward layers of the OpenELM transformer blocks as they quantize the best. Let's build a class to achieve this. \n",
    "\n",
    "## Exercise 2\n",
    "Below is a simple implementation of the Per-Channel Weight-Only Integer Quantization technique that can be applied to the PyTorch linear layer. It however is not completed, you will therefore need to fill in the missing line in the `quantize` member function, as well as the linear multiplication in the `forward` function. \n",
    "\n",
    "**NOTE**\n",
    "If you are struggling there is an example solution at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WeightOnlyInt8Linear(nn.Module): \n",
    "    def __init__(self, in_features, out_features, bias=True): \n",
    "        super().__init__()\n",
    "        self.in_features = in_features # number of input channels \n",
    "        self.out_features = out_features # number of output channels \n",
    "        self.scale = nn.Parameter(torch.ones(out_features), requires_grad=False)  # scale factor used for quantizing weights\n",
    "        # Create a tensor with the desired dtype first, then wrap it with nn.Parameter\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features, dtype=torch.int8), requires_grad=False)  # quantized weights\n",
    "        if bias: \n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features)) # bias term\n",
    "        else: \n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def quantize(self, tensor):\n",
    "        b = 8\n",
    "        qmin = -2**(b-1)\n",
    "        qmax = 2**(b-1) - 1\n",
    "        \n",
    "        # ------ Complete the missing line here ------\n",
    "        # You will need to compute a variable called `scale` that is used to quantize the `tensor` using per-channel quantization. \n",
    "        # scale = ...\n",
    "        # ---------------------------------------------\n",
    "        scale = scale.clamp(min=1e-8)\n",
    "        quantized_weights = torch.round(tensor / scale).clamp(qmin, qmax)\n",
    "        self.weight.data = quantized_weights \n",
    "        self.scale.data = scale\n",
    "        return None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ------ Complete the missing line here ------\n",
    "        # You will need to dequantize the weights `self.weight` using the scale factor `self.scale` and compute the output of the linear layer y. \n",
    "        # You can use the `torch.nn.functional.linear` function to perform the linear layer operation. (Remember to dequantize the weights first)\n",
    "        # y = ...\n",
    "        # ---------------------------------------------\n",
    "        return y\n",
    "    \n",
    "    def __repr__(self): \n",
    "        return f\"WeightOnlyInt8Linear(in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can test your implementation by running the following test. The output relative error should be similar to Exercise 1.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max relative quantization error: 40039.36 (%)\n",
      "Min relative quantization error: 0.00 (%)\n",
      "Mean relative quantization error: 7.64 (%)\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='color: green; font-size: 20px;'>Success! Mean relative quantization error: 7.64%</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# setup the full precision linear layer\n",
    "linear = nn.Linear(W_fp.shape[1], W_fp.shape[0], bias=False)\n",
    "linear.weight.data = W_fp\n",
    "\n",
    "# setup the quantized linear layer\n",
    "qlinear = WeightOnlyInt8Linear(W_fp.shape[1], W_fp.shape[0], bias=False)\n",
    "qlinear.quantize(W_fp) \n",
    "\n",
    "# run the forward pass through the linear layer and compare the output to the full precision linear layer. \n",
    "y = linear(X_fp)\n",
    "y_hat = qlinear(X_fp)\n",
    "\n",
    "# Calculate relative quantization error\n",
    "residuals = (y - y_hat).abs()\n",
    "residuals_rel_per_channel = (residuals / y.abs()) * 100\n",
    "print(f\"Max relative quantization error: {residuals_rel_per_channel.max():.2f} (%)\")\n",
    "print(f\"Min relative quantization error: {residuals_rel_per_channel.min():.2f} (%)\")\n",
    "print(f\"Mean relative quantization error: {residuals_rel_per_channel.mean():.2f} (%)\")\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "if residuals_rel_per_channel.mean() < residuals_rel.mean(): \n",
    "    display(HTML(f\"<div style='color: green; font-size: 20px;'>Success! Mean relative quantization error: {residuals_rel_per_channel.mean():.2f}%</div>\"))\n",
    "else: \n",
    "    display(HTML(f\"<div style='color: red; font-size: 20px;'>Failed. Mean relative quantization error: {residuals_rel_per_channel.mean():.2f}%. Please try again.</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now if that test has passed you can use run the following cell to quantize the OpenELM model. \n",
    "\n",
    "**Lets now use this to quantize the feedforward layers of OpenELM.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenELMForCausalLM(\n",
       "  (transformer): OpenELMModel(\n",
       "    (token_embeddings): Embedding(32000, 1280)\n",
       "    (layers): ModuleList(\n",
       "      (0): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=768, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (1): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=2048, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (2): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=2560, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (3): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=3072, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=1536, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (4): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=3584, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=1792, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (5): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=2048, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (6): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=5120, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=2560, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (7): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=5632, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=2816, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (8): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=6144, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=3072, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (9): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=6656, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=3328, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (10): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=7168, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=3584, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (11): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=7680, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=3840, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (12): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=8704, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=4352, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (13): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=9216, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=4608, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (14): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=9728, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=4864, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (15): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): WeightOnlyInt8Linear(in_features=1280, out_features=10240, bias=False)\n",
       "          (proj_2): WeightOnlyInt8Linear(in_features=5120, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-270M-Instruct\", trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "def quantize_model(model):\n",
    "    for layer_idx, block in enumerate(model.transformer.layers):\n",
    "        for name, module in block.ffn.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                qlinear = WeightOnlyInt8Linear(module.in_features, module.out_features, bias=module.bias is not None)\n",
    "                if module.bias is not None:\n",
    "                    qlinear.bias.data = module.bias.data # if a bias is used, make sure it is copied over\n",
    "                qlinear.quantize(module.weight.data.clone()) # compute the scale and quantized weights\n",
    "                \n",
    "                # Replace the module with quantized layer\n",
    "                parent_module = block.ffn # get the parent feedforward block\n",
    "                setattr(parent_module, name, qlinear)\n",
    "                \n",
    "\n",
    "quantize_model(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see in the above output of the model that the feedforward blocks named ffn, contain instances of your quantized `WeightOnlyIn8Linear` layers. Once you have done that, we can run generation on the model to compute its output and see if there are any changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm is a company that designs and manufactures microcontrollers, which are tiny electronic devices that can perform specific functions. The company has a\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Arm is a company that designs\" \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=30)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have confirmed that, let's check the static memory consumption to see what benefits weight-only quantization has brought us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Model Size Quantized: 524.4 MB\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "model.save_pretrained(\"checkpoints/OpenELM-Q8-hf\")\n",
    "pytorch_model_size_quantized = get_directory_size(\"checkpoints/OpenELM-Q8-hf\") / (1024 ** 2)\n",
    "print(f\"PyTorch Model Size Quantized: {pytorch_model_size_quantized:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. So we have reduced the static memory consumption from 1Gb to 0.5Gb for OpenELM-270M and from 1.7Gb to 0.8Gb for OpenELM-450M. This is a significant reduction and will make the model more portable to smaller devices. \n",
    "\n",
    "Let's also benchmark the generation speed as a baseline to compare to for our next section on deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Generation Speed: 23893.6 (ms)\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "prompt = \"Arm is a company that designs\" \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "st = time.time()\n",
    "outputs = model.generate(**inputs, max_length=30)\n",
    "et = time.time()\n",
    "pytorch_generation_latency = (et - st) * 1000\n",
    "\n",
    "print(f\"PyTorch Generation Speed: {pytorch_generation_latency:.1f} (ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deployment of Language Models with `llama.cpp`\n",
    "\n",
    "In this section, we explore the `llama.cpp` framework, a lightweight and efficient inference solution written in pure C++, making it well-suited for deploying language models on Arm embedded devices while leveraging advanced hardware capabilities for acceleration. Known for its minimal dependencies and streamlined design, `llama.cpp` enables fast and resource-efficient execution, making it ideal for systems with limited computational power.\n",
    "\n",
    "One key distinction between `llama.cpp` and frameworks such as Hugging Face Transformers lies in their execution modes:\n",
    "\n",
    "### Eager Mode in Hugging Face Transformers\n",
    "\n",
    "Hugging Face's Transformer models typically operate using the PyTorch framework in *eager execution mode*. In this mode:\n",
    "- Each operation is executed immediately as it is encountered, allowing for dynamic and flexible control of the computation.\n",
    "- This approach is particularly user-friendly and facilitates debugging and experimentation, as developers can inspect intermediate results and modify computations on the fly.\n",
    "- However, the trade-off is lower efficiency for inference due to repeated overhead from interpreting and dispatching operations at runtime.\n",
    "\n",
    "While eager mode is advantageous during development and for flexible use cases, its runtime inefficiency becomes a bottleneck in deployment scenarios, particularly on resource-constrained devices.\n",
    "\n",
    "### Graph Execution in `llama.cpp`\n",
    "\n",
    "In contrast, `llama.cpp` employs *graph execution*:\n",
    "- During model initialization, a computational graph is constructed, representing the sequence of operations required for inference.\n",
    "- This graph is optimized before execution, removing redundant computations, fusing operations, and leveraging hardware-specific optimizations such as vectorization or efficient memory layouts.\n",
    "- The entire graph is then executed as a single optimized unit, minimizing overhead, and maximizing throughput.\n",
    "\n",
    "This approach significantly enhances inference efficiency, particularly on embedded systems where computational and memory resources are limited. By using a static execution model, `llama.cpp` avoids the runtime overhead of dynamic operation dispatch, enabling faster and more predictable performance.\n",
    "\n",
    "\n",
    "In the following section, you will go through a workflow for deploying the OpenELM Hugging Face model with the `llama.cpp` inference framework. By transitioning from eager execution to optimized graph execution, this deployment achieves substantial performance gains, showcasing the practical advantages of `llama.cpp` for real-world applications.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 5.A Cloning the `llama.cpp` Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies \n",
    "!sudo apt install cmake g++ make\n",
    "!sudo apt install curl libcurl4-opensll-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 45218, done.\u001b[K\n",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
      "remote: Total 45218 (delta 5), reused 1 (delta 0), pack-reused 45198 (from 3)\u001b[K\n",
      "Receiving objects: 100% (45218/45218), 95.46 MiB | 6.06 MiB/s, done.\n",
      "Resolving deltas: 100% (32665/32665), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command clones the `llama.cpp` repository from GitHub to your local environment. \n",
    "\n",
    "## 5.B Building the `llama.cpp` Project with `CMake`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is GNU 12.2.0\n",
      "-- The CXX compiler identification is GNU 12.2.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.39.5\") \n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: aarch64\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
      "-- Arm detected\n",
      "-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E\n",
      "-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed\n",
      "-- Performing Test GGML_MACHINE_SUPPORTS_dotprod\n",
      "-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success\n",
      "-- Performing Test GGML_MACHINE_SUPPORTS_i8mm\n",
      "-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Failed\n",
      "-- Performing Test GGML_MACHINE_SUPPORTS_noi8mm\n",
      "-- Performing Test GGML_MACHINE_SUPPORTS_noi8mm - Success\n",
      "-- Performing Test GGML_MACHINE_SUPPORTS_sve\n",
      "-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed\n",
      "-- Performing Test GGML_MACHINE_SUPPORTS_nosve\n",
      "-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success\n",
      "-- Performing Test GGML_MACHINE_SUPPORTS_sme\n",
      "-- Performing Test GGML_MACHINE_SUPPORTS_sme - Failed\n",
      "-- Performing Test GGML_MACHINE_SUPPORTS_nosme\n",
      "-- Performing Test GGML_MACHINE_SUPPORTS_nosme - Failed\n",
      "-- Arm feature DOTPROD enabled\n",
      "-- Arm feature FMA enabled\n",
      "-- Arm feature FP16_VECTOR_ARITHMETIC enabled\n",
      "-- Adding CPU backend variant ggml-cpu: -mcpu=cortex-a76+crypto+dotprod+noi8mm+nosve \n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build\n",
      "/usr/bin/cmake -S/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp -B/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build --check-build-system CMakeFiles/Makefile.cmake 0\n",
      "/usr/bin/cmake -E cmake_progress_start /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/CMakeFiles /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build//CMakeFiles/progress.marks\n",
      "/usr/bin/gmake  -f CMakeFiles/Makefile2 all\n",
      "gmake[1]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f ggml/src/CMakeFiles/ggml-base.dir/build.make ggml/src/CMakeFiles/ggml-base.dir/depend\n",
      "/usr/bin/gmake  -f common/CMakeFiles/build_info.dir/build.make common/CMakeFiles/build_info.dir/depend\n",
      "/usr/bin/gmake  -f examples/gguf-hash/CMakeFiles/sha256.dir/build.make examples/gguf-hash/CMakeFiles/sha256.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/DependInfo.cmake --color=\n",
      "/usr/bin/gmake  -f examples/gguf-hash/CMakeFiles/xxhash.dir/build.make examples/gguf-hash/CMakeFiles/xxhash.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf-hash /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-hash /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf-hash /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-hash /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/DependInfo.cmake --color=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp && /usr/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=12.2.0 -DCMAKE_C_COMPILER_ID=GNU -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/usr/bin/cc -P /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/cmake/build-info-gen-cpp.cmake\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/gguf-hash/CMakeFiles/xxhash.dir/build.make examples/gguf-hash/CMakeFiles/xxhash.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f ggml/src/CMakeFiles/ggml-base.dir/build.make ggml/src/CMakeFiles/ggml-base.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/gguf-hash/CMakeFiles/sha256.dir/build.make examples/gguf-hash/CMakeFiles/sha256.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "-- Found Git: /usr/bin/git (found version \"2.39.5\") \n",
      "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/cc -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -std=gnu11 -MD -MT ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o -MF CMakeFiles/ggml-base.dir/ggml.c.o.d -o CMakeFiles/ggml-base.dir/ggml.c.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml.c\n",
      "[  1%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-hash && /usr/bin/cc  -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf-hash/deps -mcpu=native -O3 -DNDEBUG -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o -MF CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o.d -o CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf-hash/deps/xxhash/xxhash.c\n",
      "[  2%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-hash && /usr/bin/cc  -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf-hash/deps -mcpu=native -O3 -DNDEBUG -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o -MF CMakeFiles/sha256.dir/deps/sha256/sha256.c.o.d -o CMakeFiles/sha256.dir/deps/sha256/sha256.c.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf-hash/deps/sha256/sha256.c\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common/CMakeFiles/build_info.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f common/CMakeFiles/build_info.dir/build.make common/CMakeFiles/build_info.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[  2%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common && /usr/bin/c++   -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT common/CMakeFiles/build_info.dir/build-info.cpp.o -MF CMakeFiles/build_info.dir/build-info.cpp.o.d -o CMakeFiles/build_info.dir/build-info.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/build-info.cpp\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[  2%] Built target build_info\n",
      "/usr/bin/gmake  -f examples/gguf-hash/CMakeFiles/sha1.dir/build.make examples/gguf-hash/CMakeFiles/sha1.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf-hash /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-hash /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/gguf-hash/CMakeFiles/sha1.dir/build.make examples/gguf-hash/CMakeFiles/sha1.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[  2%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-hash && /usr/bin/cc  -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf-hash/deps -mcpu=native -O3 -DNDEBUG -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -w -MD -MT examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o -MF CMakeFiles/sha1.dir/deps/sha1/sha1.c.o.d -o CMakeFiles/sha1.dir/deps/sha1/sha1.c.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[  2%] Built target sha1\n",
      "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  3%] Built target sha256\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/cc -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -std=gnu11 -MD -MT ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o -MF CMakeFiles/ggml-base.dir/ggml-alloc.c.o.d -o CMakeFiles/ggml-base.dir/ggml-alloc.c.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-alloc.c\n",
      "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/c++ -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -std=gnu++17 -MD -MT ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -MF CMakeFiles/ggml-base.dir/ggml-backend.cpp.o.d -o CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-backend.cpp\n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/c++ -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -std=gnu++17 -MD -MT ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -MF CMakeFiles/ggml-base.dir/ggml-opt.cpp.o.d -o CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-opt.cpp\n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/c++ -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -std=gnu++17 -MD -MT ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -MF CMakeFiles/ggml-base.dir/ggml-threading.cpp.o.d -o CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-threading.cpp\n",
      "[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/cc -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -std=gnu11 -MD -MT ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -MF CMakeFiles/ggml-base.dir/ggml-quants.c.o.d -o CMakeFiles/ggml-base.dir/ggml-quants.c.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-quants.c\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[  5%] Built target xxhash\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/c++ -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -std=gnu++17 -MD -MT ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o -MF CMakeFiles/ggml-base.dir/gguf.cpp.o.d -o CMakeFiles/ggml-base.dir/gguf.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/gguf.cpp\n",
      "[  6%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/cmake -E cmake_link_script CMakeFiles/ggml-base.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -fPIC -mcpu=native -O3 -DNDEBUG -shared -Wl,-soname,libggml-base.so -o ../../bin/libggml-base.so \"CMakeFiles/ggml-base.dir/ggml.c.o\" \"CMakeFiles/ggml-base.dir/ggml-alloc.c.o\" \"CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\" \"CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\" \"CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\" \"CMakeFiles/ggml-base.dir/ggml-quants.c.o\" \"CMakeFiles/ggml-base.dir/gguf.cpp.o\"  -lm \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[  6%] Built target ggml-base\n",
      "/usr/bin/gmake  -f ggml/src/CMakeFiles/ggml-cpu.dir/build.make ggml/src/CMakeFiles/ggml-cpu.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f ggml/src/CMakeFiles/ggml-cpu.dir/build.make ggml/src/CMakeFiles/ggml-cpu.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[  7%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/cc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -mcpu=cortex-a76+crypto+dotprod+noi8mm+nosve -fopenmp -std=gnu11 -MD -MT ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -MF CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o.d -o CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -mcpu=cortex-a76+crypto+dotprod+noi8mm+nosve -fopenmp -std=gnu++17 -MD -MT ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o -MF CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o.d -o CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-hbm.cpp\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -mcpu=cortex-a76+crypto+dotprod+noi8mm+nosve -fopenmp -std=gnu++17 -MD -MT ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -MF CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o.d -o CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -mcpu=cortex-a76+crypto+dotprod+noi8mm+nosve -fopenmp -std=gnu++17 -MD -MT ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o -MF CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o.d -o CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp\n",
      "[  8%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/cc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -mcpu=cortex-a76+crypto+dotprod+noi8mm+nosve -fopenmp -std=gnu11 -MD -MT ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o -MF CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o.d -o CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-quants.c\n",
      "In function ‘\u001b[01m\u001b[Kblock_q4_0x4 make_block_q4_0x4(block_q4_0*, unsigned int)\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:39\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3614:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kwriting 8 bytes into a region of size 0 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3614 |             \u001b[01;35m\u001b[Kmemcpy(&out.qs[dst_offset], &elems, sizeof(uint64_t))\u001b[m\u001b[K;\n",
      "      |             \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:20:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kat offset 72 into destination object ‘\u001b[01m\u001b[K<anonymous>\u001b[m\u001b[K’ of size 72\n",
      " 3685 |             \u001b[01;36m\u001b[K*dst++ = make_block_q4_0x4(dst_tmp, interleave_block)\u001b[m\u001b[K;\n",
      "      |             \u001b[01;36m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "In function ‘\u001b[01m\u001b[Kblock_q4_0x4 make_block_q4_0x4(block_q4_0*, unsigned int)\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:39\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3614:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kwriting 8 bytes into a region of size 0 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3614 |             \u001b[01;35m\u001b[Kmemcpy(&out.qs[dst_offset], &elems, sizeof(uint64_t))\u001b[m\u001b[K;\n",
      "      |             \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:20:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kat offset 80 into destination object ‘\u001b[01m\u001b[K<anonymous>\u001b[m\u001b[K’ of size 72\n",
      " 3685 |             \u001b[01;36m\u001b[K*dst++ = make_block_q4_0x4(dst_tmp, interleave_block)\u001b[m\u001b[K;\n",
      "      |             \u001b[01;36m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "In function ‘\u001b[01m\u001b[Kblock_q4_0x4 make_block_q4_0x4(block_q4_0*, unsigned int)\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:39\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3614:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kwriting 8 bytes into a region of size 0 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3614 |             \u001b[01;35m\u001b[Kmemcpy(&out.qs[dst_offset], &elems, sizeof(uint64_t))\u001b[m\u001b[K;\n",
      "      |             \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:20:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kat offset 88 into destination object ‘\u001b[01m\u001b[K<anonymous>\u001b[m\u001b[K’ of size 72\n",
      " 3685 |             \u001b[01;36m\u001b[K*dst++ = make_block_q4_0x4(dst_tmp, interleave_block)\u001b[m\u001b[K;\n",
      "      |             \u001b[01;36m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "In function ‘\u001b[01m\u001b[Kblock_q4_0x4 make_block_q4_0x4(block_q4_0*, unsigned int)\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:39\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3614:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kwriting 8 bytes into a region of size 0 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3614 |             \u001b[01;35m\u001b[Kmemcpy(&out.qs[dst_offset], &elems, sizeof(uint64_t))\u001b[m\u001b[K;\n",
      "      |             \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:20:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kat offset 96 into destination object ‘\u001b[01m\u001b[K<anonymous>\u001b[m\u001b[K’ of size 72\n",
      " 3685 |             \u001b[01;36m\u001b[K*dst++ = make_block_q4_0x4(dst_tmp, interleave_block)\u001b[m\u001b[K;\n",
      "      |             \u001b[01;36m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "In function ‘\u001b[01m\u001b[Kblock_q4_0x4 make_block_q4_0x4(block_q4_0*, unsigned int)\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:39\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3614:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kwriting 8 bytes into a region of size 0 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3614 |             \u001b[01;35m\u001b[Kmemcpy(&out.qs[dst_offset], &elems, sizeof(uint64_t))\u001b[m\u001b[K;\n",
      "      |             \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:20:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kat offset 104 into destination object ‘\u001b[01m\u001b[K<anonymous>\u001b[m\u001b[K’ of size 72\n",
      " 3685 |             \u001b[01;36m\u001b[K*dst++ = make_block_q4_0x4(dst_tmp, interleave_block)\u001b[m\u001b[K;\n",
      "      |             \u001b[01;36m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "In function ‘\u001b[01m\u001b[Kblock_q4_0x4 make_block_q4_0x4(block_q4_0*, unsigned int)\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:39\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3614:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kwriting 8 bytes into a region of size 0 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3614 |             \u001b[01;35m\u001b[Kmemcpy(&out.qs[dst_offset], &elems, sizeof(uint64_t))\u001b[m\u001b[K;\n",
      "      |             \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:20:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kat offset 112 into destination object ‘\u001b[01m\u001b[K<anonymous>\u001b[m\u001b[K’ of size 72\n",
      " 3685 |             \u001b[01;36m\u001b[K*dst++ = make_block_q4_0x4(dst_tmp, interleave_block)\u001b[m\u001b[K;\n",
      "      |             \u001b[01;36m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "In function ‘\u001b[01m\u001b[Kblock_q4_0x4 make_block_q4_0x4(block_q4_0*, unsigned int)\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:39\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3614:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kwriting 8 bytes into a region of size 0 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3614 |             \u001b[01;35m\u001b[Kmemcpy(&out.qs[dst_offset], &elems, sizeof(uint64_t))\u001b[m\u001b[K;\n",
      "      |             \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:20:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kat offset 120 into destination object ‘\u001b[01m\u001b[K<anonymous>\u001b[m\u001b[K’ of size 72\n",
      " 3685 |             \u001b[01;36m\u001b[K*dst++ = make_block_q4_0x4(dst_tmp, interleave_block)\u001b[m\u001b[K;\n",
      "      |             \u001b[01;36m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "In function ‘\u001b[01m\u001b[Kblock_q4_0x4 make_block_q4_0x4(block_q4_0*, unsigned int)\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:39\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3614:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kwriting 8 bytes into a region of size 0 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3614 |             \u001b[01;35m\u001b[Kmemcpy(&out.qs[dst_offset], &elems, sizeof(uint64_t))\u001b[m\u001b[K;\n",
      "      |             \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp:3685:20:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kat offset 128 into destination object ‘\u001b[01m\u001b[K<anonymous>\u001b[m\u001b[K’ of size 72\n",
      " 3685 |             \u001b[01;36m\u001b[K*dst++ = make_block_q4_0x4(dst_tmp, interleave_block)\u001b[m\u001b[K;\n",
      "      |             \u001b[01;36m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -mcpu=cortex-a76+crypto+dotprod+noi8mm+nosve -fopenmp -std=gnu++17 -MD -MT ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o -MF CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o.d -o CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-traits.cpp\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -mcpu=cortex-a76+crypto+dotprod+noi8mm+nosve -fopenmp -std=gnu++17 -MD -MT ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -MF CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o.d -o CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -mcpu=cortex-a76+crypto+dotprod+noi8mm+nosve -fopenmp -std=gnu++17 -MD -MT ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -MF CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o.d -o CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp\n",
      "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -mcpu=cortex-a76+crypto+dotprod+noi8mm+nosve -fopenmp -std=gnu++17 -MD -MT ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -MF CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o.d -o CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.cpp\n",
      "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/cmake -E cmake_link_script CMakeFiles/ggml-cpu.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -fPIC -mcpu=native -O3 -DNDEBUG -shared -Wl,-soname,libggml-cpu.so -o ../../bin/libggml-cpu.so \"CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\" \"CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\" \"CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\" \"CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\" \"CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\" \"CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\" \"CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\" \"CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\" \"CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\"  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../bin/libggml-base.so /usr/lib/gcc/aarch64-linux-gnu/12/libgomp.so /usr/lib/aarch64-linux-gnu/libpthread.a \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 10%] Built target ggml-cpu\n",
      "/usr/bin/gmake  -f ggml/src/CMakeFiles/ggml.dir/build.make ggml/src/CMakeFiles/ggml.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f ggml/src/CMakeFiles/ggml.dir/build.make ggml/src/CMakeFiles/ggml.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -std=gnu++17 -MD -MT ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o -MF CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o.d -o CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/ggml-backend-reg.cpp\n",
      "[ 11%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/ggml/src && /usr/bin/cmake -E cmake_link_script CMakeFiles/ggml.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -fPIC -mcpu=native -O3 -DNDEBUG -shared -Wl,-soname,libggml.so -o ../../bin/libggml.so \"CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\"  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: -ldl ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 11%] Built target ggml\n",
      "/usr/bin/gmake  -f src/CMakeFiles/llama.dir/build.make src/CMakeFiles/llama.dir/depend\n",
      "/usr/bin/gmake  -f examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/build.make examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src/CMakeFiles/llama.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/gguf/CMakeFiles/llama-gguf.dir/build.make examples/gguf/CMakeFiles/llama-gguf.dir/depend\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf-hash /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-hash /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f src/CMakeFiles/llama.dir/build.make src/CMakeFiles/llama.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/build.make examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/gguf/CMakeFiles/llama-gguf.dir/build.make examples/gguf/CMakeFiles/llama-gguf.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama.cpp.o -MF CMakeFiles/llama.dir/llama.cpp.o.d -o CMakeFiles/llama.dir/llama.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama.cpp\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-hash && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf-hash/deps -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o -MF CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o.d -o CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf-hash/gguf-hash.cpp\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-adapter.cpp.o -MF CMakeFiles/llama.dir/llama-adapter.cpp.o.d -o CMakeFiles/llama.dir/llama-adapter.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-adapter.cpp\n",
      "[ 14%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o -MF CMakeFiles/llama-gguf.dir/gguf.cpp.o.d -o CMakeFiles/llama-gguf.dir/gguf.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf/gguf.cpp\n",
      "[ 14%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-gguf.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-gguf.dir/gguf.cpp.o\" -o ../../bin/llama-gguf  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 14%] Built target llama-gguf\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-arch.cpp.o -MF CMakeFiles/llama.dir/llama-arch.cpp.o.d -o CMakeFiles/llama.dir/llama-arch.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-arch.cpp\n",
      "[ 15%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-hash && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-gguf-hash.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\" CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o CMakeFiles/sha1.dir/deps/sha1/sha1.c.o CMakeFiles/sha256.dir/deps/sha256/sha256.c.o -o ../../bin/llama-gguf-hash  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 15%] Built target llama-gguf-hash\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-batch.cpp.o -MF CMakeFiles/llama.dir/llama-batch.cpp.o.d -o CMakeFiles/llama.dir/llama-batch.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-batch.cpp\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-chat.cpp.o -MF CMakeFiles/llama.dir/llama-chat.cpp.o.d -o CMakeFiles/llama.dir/llama-chat.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-chat.cpp\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-context.cpp.o -MF CMakeFiles/llama.dir/llama-context.cpp.o.d -o CMakeFiles/llama.dir/llama-context.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-context.cpp\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-grammar.cpp.o -MF CMakeFiles/llama.dir/llama-grammar.cpp.o.d -o CMakeFiles/llama.dir/llama-grammar.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-grammar.cpp\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-hparams.cpp.o -MF CMakeFiles/llama.dir/llama-hparams.cpp.o.d -o CMakeFiles/llama.dir/llama-hparams.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-hparams.cpp\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-impl.cpp.o -MF CMakeFiles/llama.dir/llama-impl.cpp.o.d -o CMakeFiles/llama.dir/llama-impl.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-impl.cpp\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o -MF CMakeFiles/llama.dir/llama-kv-cache.cpp.o.d -o CMakeFiles/llama.dir/llama-kv-cache.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-kv-cache.cpp\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-mmap.cpp.o -MF CMakeFiles/llama.dir/llama-mmap.cpp.o.d -o CMakeFiles/llama.dir/llama-mmap.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-mmap.cpp\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-model-loader.cpp.o -MF CMakeFiles/llama.dir/llama-model-loader.cpp.o.d -o CMakeFiles/llama.dir/llama-model-loader.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-model-loader.cpp\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-model.cpp.o -MF CMakeFiles/llama.dir/llama-model.cpp.o.d -o CMakeFiles/llama.dir/llama-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-model.cpp\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-quant.cpp.o -MF CMakeFiles/llama.dir/llama-quant.cpp.o.d -o CMakeFiles/llama.dir/llama-quant.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-quant.cpp\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-sampling.cpp.o -MF CMakeFiles/llama.dir/llama-sampling.cpp.o.d -o CMakeFiles/llama.dir/llama-sampling.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-sampling.cpp\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/llama-vocab.cpp.o -MF CMakeFiles/llama.dir/llama-vocab.cpp.o.d -o CMakeFiles/llama.dir/llama-vocab.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/llama-vocab.cpp\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/unicode.cpp.o -MF CMakeFiles/llama.dir/unicode.cpp.o.d -o CMakeFiles/llama.dir/unicode.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/unicode.cpp\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT src/CMakeFiles/llama.dir/unicode-data.cpp.o -MF CMakeFiles/llama.dir/unicode-data.cpp.o.d -o CMakeFiles/llama.dir/unicode-data.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/unicode-data.cpp\n",
      "[ 23%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/src && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -fPIC -mcpu=native -O3 -DNDEBUG -shared -Wl,-soname,libllama.so -o ../bin/libllama.so CMakeFiles/llama.dir/llama.cpp.o \"CMakeFiles/llama.dir/llama-adapter.cpp.o\" \"CMakeFiles/llama.dir/llama-arch.cpp.o\" \"CMakeFiles/llama.dir/llama-batch.cpp.o\" \"CMakeFiles/llama.dir/llama-chat.cpp.o\" \"CMakeFiles/llama.dir/llama-context.cpp.o\" \"CMakeFiles/llama.dir/llama-grammar.cpp.o\" \"CMakeFiles/llama.dir/llama-hparams.cpp.o\" \"CMakeFiles/llama.dir/llama-impl.cpp.o\" \"CMakeFiles/llama.dir/llama-kv-cache.cpp.o\" \"CMakeFiles/llama.dir/llama-mmap.cpp.o\" \"CMakeFiles/llama.dir/llama-model-loader.cpp.o\" \"CMakeFiles/llama.dir/llama-model.cpp.o\" \"CMakeFiles/llama.dir/llama-quant.cpp.o\" \"CMakeFiles/llama.dir/llama-sampling.cpp.o\" \"CMakeFiles/llama.dir/llama-vocab.cpp.o\" CMakeFiles/llama.dir/unicode.cpp.o \"CMakeFiles/llama.dir/unicode-data.cpp.o\"  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 23%] Built target llama\n",
      "/usr/bin/gmake  -f common/CMakeFiles/common.dir/build.make common/CMakeFiles/common.dir/depend\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-c.dir/build.make tests/CMakeFiles/test-c.dir/depend\n",
      "/usr/bin/gmake  -f examples/simple/CMakeFiles/llama-simple.dir/build.make examples/simple/CMakeFiles/llama-simple.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-c.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common/CMakeFiles/common.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/simple-chat/CMakeFiles/llama-simple-chat.dir/build.make examples/simple-chat/CMakeFiles/llama-simple-chat.dir/depend\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/simple /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/simple /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/simple-chat /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/simple-chat /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-c.dir/build.make tests/CMakeFiles/test-c.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f common/CMakeFiles/common.dir/build.make common/CMakeFiles/common.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/simple/CMakeFiles/llama-simple.dir/build.make examples/simple/CMakeFiles/llama-simple.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/simple-chat/CMakeFiles/llama-simple-chat.dir/build.make examples/simple-chat/CMakeFiles/llama-simple-chat.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 24%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cc -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT tests/CMakeFiles/test-c.dir/test-c.c.o -MF CMakeFiles/test-c.dir/test-c.c.o.d -o CMakeFiles/test-c.dir/test-c.c.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-c.c\n",
      "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT common/CMakeFiles/common.dir/arg.cpp.o -MF CMakeFiles/common.dir/arg.cpp.o.d -o CMakeFiles/common.dir/arg.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/arg.cpp\n",
      "[ 25%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/simple && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o -MF CMakeFiles/llama-simple.dir/simple.cpp.o.d -o CMakeFiles/llama-simple.dir/simple.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/simple/simple.cpp\n",
      "[ 25%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/simple-chat && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o -MF CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o.d -o CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/simple-chat/simple-chat.cpp\n",
      "[ 25%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-c.dir/link.txt --verbose=1\n",
      "/usr/bin/cc -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-c.dir/test-c.c.o\" -o ../bin/test-c  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 25%] Built target test-c\n",
      "/usr/bin/gmake  -f examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/build.make examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/quantize-stats /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/quantize-stats /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/build.make examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 25%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/quantize-stats && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/quantize-stats/../../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o -MF CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o.d -o CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/quantize-stats/quantize-stats.cpp\n",
      "[ 26%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/simple && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-simple.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-simple.dir/simple.cpp.o\" -o ../../bin/llama-simple  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 26%] Built target llama-simple\n",
      "/usr/bin/gmake  -f examples/llava/CMakeFiles/llava.dir/build.make examples/llava/CMakeFiles/llava.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/llava/CMakeFiles/llava.dir/build.make examples/llava/CMakeFiles/llava.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 26%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/../.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/../../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-cast-qual -MD -MT examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF CMakeFiles/llava.dir/llava.cpp.o.d -o CMakeFiles/llava.dir/llava.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/llava.cpp\n",
      "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/simple-chat && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-simple-chat.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\" -o ../../bin/llama-simple-chat  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 27%] Built target llama-simple-chat\n",
      "[ 27%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT common/CMakeFiles/common.dir/chat.cpp.o -MF CMakeFiles/common.dir/chat.cpp.o.d -o CMakeFiles/common.dir/chat.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/chat.cpp\n",
      "[ 28%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/../.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/../../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-cast-qual -MD -MT examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF CMakeFiles/llava.dir/clip.cpp.o.d -o CMakeFiles/llava.dir/clip.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/clip.cpp\n",
      "[ 29%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize-stats\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/quantize-stats && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-quantize-stats.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o\" \"../../common/CMakeFiles/build_info.dir/build-info.cpp.o\" -o ../../bin/llama-quantize-stats  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 29%] Built target llama-quantize-stats\n",
      "[ 30%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT common/CMakeFiles/common.dir/common.cpp.o -MF CMakeFiles/common.dir/common.cpp.o.d -o CMakeFiles/common.dir/common.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/common.cpp\n",
      "[ 30%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT common/CMakeFiles/common.dir/console.cpp.o -MF CMakeFiles/common.dir/console.cpp.o.d -o CMakeFiles/common.dir/console.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/console.cpp\n",
      "[ 31%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -MF CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d -o CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/json-schema-to-grammar.cpp\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 31%] Built target llava\n",
      "[ 31%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT common/CMakeFiles/common.dir/llguidance.cpp.o -MF CMakeFiles/common.dir/llguidance.cpp.o.d -o CMakeFiles/common.dir/llguidance.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/llguidance.cpp\n",
      "[ 32%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT common/CMakeFiles/common.dir/log.cpp.o -MF CMakeFiles/common.dir/log.cpp.o.d -o CMakeFiles/common.dir/log.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/log.cpp\n",
      "[ 32%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT common/CMakeFiles/common.dir/ngram-cache.cpp.o -MF CMakeFiles/common.dir/ngram-cache.cpp.o.d -o CMakeFiles/common.dir/ngram-cache.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/ngram-cache.cpp\n",
      "[ 33%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT common/CMakeFiles/common.dir/sampling.cpp.o -MF CMakeFiles/common.dir/sampling.cpp.o.d -o CMakeFiles/common.dir/sampling.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/sampling.cpp\n",
      "[ 33%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT common/CMakeFiles/common.dir/speculative.cpp.o -MF CMakeFiles/common.dir/speculative.cpp.o.d -o CMakeFiles/common.dir/speculative.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/speculative.cpp\n",
      "/usr/bin/gmake  -f examples/llava/CMakeFiles/llava_static.dir/build.make examples/llava/CMakeFiles/llava_static.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/llava/CMakeFiles/llava_static.dir/build.make examples/llava/CMakeFiles/llava_static.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 34%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava && /usr/bin/cmake -P CMakeFiles/llava_static.dir/cmake_clean_target.cmake\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava && /usr/bin/cmake -E cmake_link_script CMakeFiles/llava_static.dir/link.txt --verbose=1\n",
      "/usr/bin/ar qc libllava_static.a CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o\n",
      "/usr/bin/ranlib libllava_static.a\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 34%] Built target llava_static\n",
      "/usr/bin/gmake  -f examples/llava/CMakeFiles/llava_shared.dir/build.make examples/llava/CMakeFiles/llava_shared.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/llava/CMakeFiles/llava_shared.dir/build.make examples/llava/CMakeFiles/llava_shared.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 34%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libllava_shared.so\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava && /usr/bin/cmake -E cmake_link_script CMakeFiles/llava_shared.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -fPIC -mcpu=native -O3 -DNDEBUG -shared -Wl,-soname,libllava_shared.so -o ../../bin/libllava_shared.so CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 34%] Built target llava_shared\n",
      "[ 35%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common && /usr/bin/cmake -P CMakeFiles/common.dir/cmake_clean_target.cmake\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/common && /usr/bin/cmake -E cmake_link_script CMakeFiles/common.dir/link.txt --verbose=1\n",
      "/usr/bin/ar qc libcommon.a CMakeFiles/common.dir/arg.cpp.o CMakeFiles/common.dir/chat.cpp.o CMakeFiles/common.dir/common.cpp.o CMakeFiles/common.dir/console.cpp.o \"CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\" CMakeFiles/common.dir/llguidance.cpp.o CMakeFiles/common.dir/log.cpp.o \"CMakeFiles/common.dir/ngram-cache.cpp.o\" CMakeFiles/common.dir/sampling.cpp.o CMakeFiles/common.dir/speculative.cpp.o \"CMakeFiles/build_info.dir/build-info.cpp.o\"\n",
      "/usr/bin/ranlib libcommon.a\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 35%] Built target common\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-tokenizer-0.dir/build.make tests/CMakeFiles/test-tokenizer-0.dir/depend\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-sampling.dir/build.make tests/CMakeFiles/test-sampling.dir/depend\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-grammar-parser.dir/build.make tests/CMakeFiles/test-grammar-parser.dir/depend\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-grammar-integration.dir/build.make tests/CMakeFiles/test-grammar-integration.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/DependInfo.cmake --color=\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-grammar-integration.dir/build.make tests/CMakeFiles/test-grammar-integration.dir/build\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-sampling.dir/build.make tests/CMakeFiles/test-sampling.dir/build\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-grammar-parser.dir/build.make tests/CMakeFiles/test-grammar-parser.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-tokenizer-0.dir/build.make tests/CMakeFiles/test-tokenizer-0.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o -MF CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o.d -o CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-grammar-parser.cpp\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o -MF CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o.d -o CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-grammar-integration.cpp\n",
      "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o -MF CMakeFiles/test-sampling.dir/test-sampling.cpp.o.d -o CMakeFiles/test-sampling.dir/test-sampling.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-sampling.cpp\n",
      "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o -MF CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o.d -o CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-tokenizer-0.cpp\n",
      "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o -MF CMakeFiles/test-grammar-parser.dir/get-model.cpp.o.d -o CMakeFiles/test-grammar-parser.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "[ 38%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-grammar-parser.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\" \"CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\" -o ../bin/test-grammar-parser  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 38%] Built target test-grammar-parser\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-llama-grammar.dir/build.make tests/CMakeFiles/test-llama-grammar.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-llama-grammar.dir/build.make tests/CMakeFiles/test-llama-grammar.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o -MF CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o.d -o CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-llama-grammar.cpp\n",
      "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-sampling.dir/get-model.cpp.o -MF CMakeFiles/test-sampling.dir/get-model.cpp.o.d -o CMakeFiles/test-sampling.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-tokenizer-0.dir/link.txt --verbose=1\n",
      "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-sampling.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\" -o ../bin/test-tokenizer-0  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-sampling.dir/test-sampling.cpp.o\" \"CMakeFiles/test-sampling.dir/get-model.cpp.o\" -o ../bin/test-sampling  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 39%] Built target test-tokenizer-0\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-chat.dir/build.make tests/CMakeFiles/test-chat.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-chat.dir/DependInfo.cmake --color=\n",
      "[ 39%] Built target test-sampling\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-json-schema-to-grammar.dir/build.make tests/CMakeFiles/test-json-schema-to-grammar.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-json-schema-to-grammar.dir/build.make tests/CMakeFiles/test-json-schema-to-grammar.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-chat.dir/build.make tests/CMakeFiles/test-chat.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/../examples/server -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o -MF CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o.d -o CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-json-schema-to-grammar.cpp\n",
      "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-chat.dir/test-chat.cpp.o -MF CMakeFiles/test-chat.dir/test-chat.cpp.o.d -o CMakeFiles/test-chat.dir/test-chat.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-chat.cpp\n",
      "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o -MF CMakeFiles/test-llama-grammar.dir/get-model.cpp.o.d -o CMakeFiles/test-llama-grammar.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-llama-grammar.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\" \"CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\" -o ../bin/test-llama-grammar  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 41%] Built target test-llama-grammar\n",
      "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/../examples/server -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o -MF CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o.d -o CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-chat.dir/get-model.cpp.o -MF CMakeFiles/test-chat.dir/get-model.cpp.o.d -o CMakeFiles/test-chat.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-tokenizer-1-bpe.dir/build.make tests/CMakeFiles/test-tokenizer-1-bpe.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-tokenizer-1-bpe.dir/build.make tests/CMakeFiles/test-tokenizer-1-bpe.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o -MF CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o.d -o CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-tokenizer-1-bpe.cpp\n",
      "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-tokenizer-1-bpe.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\" -o ../bin/test-tokenizer-1-bpe  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 42%] Built target test-tokenizer-1-bpe\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-tokenizer-1-spm.dir/build.make tests/CMakeFiles/test-tokenizer-1-spm.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-tokenizer-1-spm.dir/build.make tests/CMakeFiles/test-tokenizer-1-spm.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o -MF CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o.d -o CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-tokenizer-1-spm.cpp\n",
      "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-tokenizer-1-spm.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\" -o ../bin/test-tokenizer-1-spm  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 43%] Built target test-tokenizer-1-spm\n",
      "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o -MF CMakeFiles/test-grammar-integration.dir/get-model.cpp.o.d -o CMakeFiles/test-grammar-integration.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-log.dir/build.make tests/CMakeFiles/test-log.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-log.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-log.dir/build.make tests/CMakeFiles/test-log.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-log.dir/test-log.cpp.o -MF CMakeFiles/test-log.dir/test-log.cpp.o.d -o CMakeFiles/test-log.dir/test-log.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-log.cpp\n",
      "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-log.dir/get-model.cpp.o -MF CMakeFiles/test-log.dir/get-model.cpp.o.d -o CMakeFiles/test-log.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-log.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-log.dir/test-log.cpp.o\" \"CMakeFiles/test-log.dir/get-model.cpp.o\" -o ../bin/test-log  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 46%] Built target test-log\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-arg-parser.dir/build.make tests/CMakeFiles/test-arg-parser.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-arg-parser.dir/build.make tests/CMakeFiles/test-arg-parser.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o -MF CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o.d -o CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-arg-parser.cpp\n",
      "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o -MF CMakeFiles/test-arg-parser.dir/get-model.cpp.o.d -o CMakeFiles/test-arg-parser.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-arg-parser.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\" \"CMakeFiles/test-arg-parser.dir/get-model.cpp.o\" -o ../bin/test-arg-parser  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 47%] Built target test-arg-parser\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-chat-template.dir/build.make tests/CMakeFiles/test-chat-template.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-chat-template.dir/build.make tests/CMakeFiles/test-chat-template.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o -MF CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o.d -o CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-chat-template.cpp\n",
      "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-grammar-integration.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\" \"CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\" -o ../bin/test-grammar-integration  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 47%] Built target test-grammar-integration\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-gguf.dir/build.make tests/CMakeFiles/test-gguf.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-gguf.dir/build.make tests/CMakeFiles/test-gguf.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o -MF CMakeFiles/test-gguf.dir/test-gguf.cpp.o.d -o CMakeFiles/test-gguf.dir/test-gguf.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-gguf.cpp\n",
      "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-chat.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-chat.dir/test-chat.cpp.o\" \"CMakeFiles/test-chat.dir/get-model.cpp.o\" -o ../bin/test-chat  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 49%] Built target test-chat\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-backend-ops.dir/build.make tests/CMakeFiles/test-backend-ops.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-backend-ops.dir/build.make tests/CMakeFiles/test-backend-ops.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o -MF CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o.d -o CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-backend-ops.cpp\n",
      "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o -MF CMakeFiles/test-chat-template.dir/get-model.cpp.o.d -o CMakeFiles/test-chat-template.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-chat-template.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\" \"CMakeFiles/test-chat-template.dir/get-model.cpp.o\" -o ../bin/test-chat-template  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-gguf.dir/get-model.cpp.o -MF CMakeFiles/test-gguf.dir/get-model.cpp.o.d -o CMakeFiles/test-gguf.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "[ 51%] Built target test-chat-template\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-model-load-cancel.dir/build.make tests/CMakeFiles/test-model-load-cancel.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-model-load-cancel.dir/build.make tests/CMakeFiles/test-model-load-cancel.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o -MF CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o.d -o CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-model-load-cancel.cpp\n",
      "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-gguf.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-gguf.dir/test-gguf.cpp.o\" \"CMakeFiles/test-gguf.dir/get-model.cpp.o\" -o ../bin/test-gguf  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o -MF CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o.d -o CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 53%] Built target test-gguf\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-autorelease.dir/build.make tests/CMakeFiles/test-autorelease.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-autorelease.dir/build.make tests/CMakeFiles/test-autorelease.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o -MF CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o.d -o CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-autorelease.cpp\n",
      "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-model-load-cancel.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\" \"CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\" -o ../bin/test-model-load-cancel  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 53%] Built target test-model-load-cancel\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-barrier.dir/build.make tests/CMakeFiles/test-barrier.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-barrier.dir/build.make tests/CMakeFiles/test-barrier.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o -MF CMakeFiles/test-barrier.dir/test-barrier.cpp.o.d -o CMakeFiles/test-barrier.dir/test-barrier.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-barrier.cpp\n",
      "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o -MF CMakeFiles/test-autorelease.dir/get-model.cpp.o.d -o CMakeFiles/test-autorelease.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-autorelease.dir/link.txt --verbose=1\n",
      "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-json-schema-to-grammar.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\" \"CMakeFiles/test-autorelease.dir/get-model.cpp.o\" -o ../bin/test-autorelease  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\" \"CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\" -o ../bin/test-json-schema-to-grammar  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 55%] Built target test-json-schema-to-grammar\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o -MF CMakeFiles/test-backend-ops.dir/get-model.cpp.o.d -o CMakeFiles/test-backend-ops.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "[ 55%] Built target test-autorelease\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-quantize-fns.dir/build.make tests/CMakeFiles/test-quantize-fns.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-quantize-fns.dir/build.make tests/CMakeFiles/test-quantize-fns.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-barrier.dir/get-model.cpp.o -MF CMakeFiles/test-barrier.dir/get-model.cpp.o.d -o CMakeFiles/test-barrier.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o -MF CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o.d -o CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-quantize-fns.cpp\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o -MF CMakeFiles/test-quantize-fns.dir/get-model.cpp.o.d -o CMakeFiles/test-quantize-fns.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-quantize-perf.dir/build.make tests/CMakeFiles/test-quantize-perf.dir/depend\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-barrier.dir/link.txt --verbose=1\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/DependInfo.cmake --color=\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-barrier.dir/test-barrier.cpp.o\" \"CMakeFiles/test-barrier.dir/get-model.cpp.o\" -o ../bin/test-barrier  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-quantize-perf.dir/build.make tests/CMakeFiles/test-quantize-perf.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o -MF CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o.d -o CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-quantize-perf.cpp\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 57%] Built target test-barrier\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-rope.dir/build.make tests/CMakeFiles/test-rope.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests/CMakeFiles/test-rope.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f tests/CMakeFiles/test-rope.dir/build.make tests/CMakeFiles/test-rope.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-rope.dir/test-rope.cpp.o -MF CMakeFiles/test-rope.dir/test-rope.cpp.o.d -o CMakeFiles/test-rope.dir/test-rope.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/test-rope.cpp\n",
      "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-rope.dir/get-model.cpp.o -MF CMakeFiles/test-rope.dir/get-model.cpp.o.d -o CMakeFiles/test-rope.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-quantize-fns.dir/link.txt --verbose=1\n",
      "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-rope.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\" \"CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\" -o ../bin/test-quantize-fns  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-rope.dir/test-rope.cpp.o\" \"CMakeFiles/test-rope.dir/get-model.cpp.o\" -o ../bin/test-rope  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 60%] Built target test-rope\n",
      "/usr/bin/gmake  -f examples/batched-bench/CMakeFiles/llama-batched-bench.dir/build.make examples/batched-bench/CMakeFiles/llama-batched-bench.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/batched-bench /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/batched-bench /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/batched-bench/CMakeFiles/llama-batched-bench.dir/build.make examples/batched-bench/CMakeFiles/llama-batched-bench.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 60%] Built target test-quantize-fns\n",
      "[ 60%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
      "/usr/bin/gmake  -f examples/batched/CMakeFiles/llama-batched.dir/build.make examples/batched/CMakeFiles/llama-batched.dir/depend\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/batched-bench && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o -MF CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o.d -o CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/batched-bench/batched-bench.cpp\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/batched /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/batched /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/batched/CMakeFiles/llama-batched.dir/build.make examples/batched/CMakeFiles/llama-batched.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 60%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/batched && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o -MF CMakeFiles/llama-batched.dir/batched.cpp.o.d -o CMakeFiles/llama-batched.dir/batched.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/batched/batched.cpp\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o -MF CMakeFiles/test-quantize-perf.dir/get-model.cpp.o.d -o CMakeFiles/test-quantize-perf.dir/get-model.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/tests/get-model.cpp\n",
      "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-quantize-perf.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\" \"CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\" -o ../bin/test-quantize-perf  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 61%] Built target test-quantize-perf\n",
      "/usr/bin/gmake  -f examples/embedding/CMakeFiles/llama-embedding.dir/build.make examples/embedding/CMakeFiles/llama-embedding.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/embedding /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/embedding /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/embedding/CMakeFiles/llama-embedding.dir/build.make examples/embedding/CMakeFiles/llama-embedding.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 62%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/embedding && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o -MF CMakeFiles/llama-embedding.dir/embedding.cpp.o.d -o CMakeFiles/llama-embedding.dir/embedding.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/embedding/embedding.cpp\n",
      "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/batched-bench && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-batched-bench.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\" -o ../../bin/llama-batched-bench  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 63%] Built target llama-batched-bench\n",
      "/usr/bin/gmake  -f examples/eval-callback/CMakeFiles/llama-eval-callback.dir/build.make examples/eval-callback/CMakeFiles/llama-eval-callback.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/eval-callback /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/eval-callback /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/eval-callback/CMakeFiles/llama-eval-callback.dir/build.make examples/eval-callback/CMakeFiles/llama-eval-callback.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 64%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/eval-callback && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o -MF CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o.d -o CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/eval-callback/eval-callback.cpp\n",
      "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/batched && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-batched.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-batched.dir/batched.cpp.o\" -o ../../bin/llama-batched  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 65%] Built target llama-batched\n",
      "/usr/bin/gmake  -f examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/build.make examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gbnf-validator /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gbnf-validator /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/build.make examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 66%] \u001b[32mBuilding CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gbnf-validator && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o -MF CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o.d -o CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gbnf-validator/gbnf-validator.cpp\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gbnf-validator\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gbnf-validator && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-gbnf-validator.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o\" -o ../../bin/llama-gbnf-validator  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 66%] Built target llama-gbnf-validator\n",
      "/usr/bin/gmake  -f examples/gguf-split/CMakeFiles/llama-gguf-split.dir/build.make examples/gguf-split/CMakeFiles/llama-gguf-split.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf-split /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-split /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/gguf-split/CMakeFiles/llama-gguf-split.dir/build.make examples/gguf-split/CMakeFiles/llama-gguf-split.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 67%] \u001b[32mBuilding CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-split && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o -MF CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o.d -o CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gguf-split/gguf-split.cpp\n",
      "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/embedding && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-embedding.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-embedding.dir/embedding.cpp.o\" -o ../../bin/llama-embedding  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/eval-callback && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-eval-callback.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\" -o ../../bin/llama-eval-callback  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 67%] Built target llama-embedding\n",
      "/usr/bin/gmake  -f examples/gritlm/CMakeFiles/llama-gritlm.dir/build.make examples/gritlm/CMakeFiles/llama-gritlm.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gritlm /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gritlm /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/gritlm/CMakeFiles/llama-gritlm.dir/build.make examples/gritlm/CMakeFiles/llama-gritlm.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 68%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gritlm && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o -MF CMakeFiles/llama-gritlm.dir/gritlm.cpp.o.d -o CMakeFiles/llama-gritlm.dir/gritlm.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gritlm/gritlm.cpp\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 68%] Built target llama-eval-callback\n",
      "/usr/bin/gmake  -f examples/imatrix/CMakeFiles/llama-imatrix.dir/build.make examples/imatrix/CMakeFiles/llama-imatrix.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/imatrix /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/imatrix /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/imatrix/CMakeFiles/llama-imatrix.dir/build.make examples/imatrix/CMakeFiles/llama-imatrix.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 69%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/imatrix && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o -MF CMakeFiles/llama-imatrix.dir/imatrix.cpp.o.d -o CMakeFiles/llama-imatrix.dir/imatrix.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/imatrix/imatrix.cpp\n",
      "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gguf-split && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-gguf-split.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\" -o ../../bin/llama-gguf-split  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 69%] Built target llama-gguf-split\n",
      "/usr/bin/gmake  -f examples/infill/CMakeFiles/llama-infill.dir/build.make examples/infill/CMakeFiles/llama-infill.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/infill /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/infill /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/infill/CMakeFiles/llama-infill.dir/build.make examples/infill/CMakeFiles/llama-infill.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 70%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/infill && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o -MF CMakeFiles/llama-infill.dir/infill.cpp.o.d -o CMakeFiles/llama-infill.dir/infill.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/infill/infill.cpp\n",
      "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gritlm && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-gritlm.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\" -o ../../bin/llama-gritlm  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 70%] Built target llama-gritlm\n",
      "/usr/bin/gmake  -f examples/llama-bench/CMakeFiles/llama-bench.dir/build.make examples/llama-bench/CMakeFiles/llama-bench.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llama-bench /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llama-bench /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/llama-bench/CMakeFiles/llama-bench.dir/build.make examples/llama-bench/CMakeFiles/llama-bench.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 70%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llama-bench && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o -MF CMakeFiles/llama-bench.dir/llama-bench.cpp.o.d -o CMakeFiles/llama-bench.dir/llama-bench.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llama-bench/llama-bench.cpp\n",
      "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-infill\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/infill && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-infill.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-infill.dir/infill.cpp.o\" -o ../../bin/llama-infill  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 70%] Built target llama-infill\n",
      "/usr/bin/gmake  -f examples/lookahead/CMakeFiles/llama-lookahead.dir/build.make examples/lookahead/CMakeFiles/llama-lookahead.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/lookahead /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookahead /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/lookahead/CMakeFiles/llama-lookahead.dir/build.make examples/lookahead/CMakeFiles/llama-lookahead.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 70%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookahead && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o -MF CMakeFiles/llama-lookahead.dir/lookahead.cpp.o.d -o CMakeFiles/llama-lookahead.dir/lookahead.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/lookahead/lookahead.cpp\n",
      "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/imatrix && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-imatrix.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\" -o ../../bin/llama-imatrix  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 70%] Built target llama-imatrix\n",
      "/usr/bin/gmake  -f examples/lookup/CMakeFiles/llama-lookup.dir/build.make examples/lookup/CMakeFiles/llama-lookup.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/lookup /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/lookup/CMakeFiles/llama-lookup.dir/build.make examples/lookup/CMakeFiles/llama-lookup.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 70%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o -MF CMakeFiles/llama-lookup.dir/lookup.cpp.o.d -o CMakeFiles/llama-lookup.dir/lookup.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/lookup/lookup.cpp\n",
      "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookahead && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-lookahead.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\" -o ../../bin/llama-lookahead  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 71%] Built target llama-lookahead\n",
      "/usr/bin/gmake  -f examples/lookup/CMakeFiles/llama-lookup-create.dir/build.make examples/lookup/CMakeFiles/llama-lookup-create.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/lookup /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/lookup/CMakeFiles/llama-lookup-create.dir/build.make examples/lookup/CMakeFiles/llama-lookup-create.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 71%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o -MF CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o.d -o CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/lookup/lookup-create.cpp\n",
      "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-lookup.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-lookup.dir/lookup.cpp.o\" -o ../../bin/llama-lookup  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 72%] Built target llama-lookup\n",
      "/usr/bin/gmake  -f examples/lookup/CMakeFiles/llama-lookup-merge.dir/build.make examples/lookup/CMakeFiles/llama-lookup-merge.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/lookup /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/lookup/CMakeFiles/llama-lookup-merge.dir/build.make examples/lookup/CMakeFiles/llama-lookup-merge.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 72%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o -MF CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o.d -o CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/lookup/lookup-merge.cpp\n",
      "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-lookup-create.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\" -o ../../bin/llama-lookup-create  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 73%] Built target llama-lookup-create\n",
      "/usr/bin/gmake  -f examples/lookup/CMakeFiles/llama-lookup-stats.dir/build.make examples/lookup/CMakeFiles/llama-lookup-stats.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/lookup /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/lookup/CMakeFiles/llama-lookup-stats.dir/build.make examples/lookup/CMakeFiles/llama-lookup-stats.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 73%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o -MF CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o.d -o CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/lookup/lookup-stats.cpp\n",
      "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/tests && /usr/bin/cmake -E cmake_link_script CMakeFiles/test-backend-ops.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\" \"CMakeFiles/test-backend-ops.dir/get-model.cpp.o\" -o ../bin/test-backend-ops  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../common/libcommon.a ../bin/libllama.so ../bin/libggml.so ../bin/libggml-cpu.so ../bin/libggml-base.so \n",
      "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-lookup-merge.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\" -o ../../bin/llama-lookup-merge  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 75%] Built target test-backend-ops\n",
      "/usr/bin/gmake  -f examples/main/CMakeFiles/llama-cli.dir/build.make examples/main/CMakeFiles/llama-cli.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/main /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/main /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/main/CMakeFiles/llama-cli.dir/build.make examples/main/CMakeFiles/llama-cli.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 75%] Built target llama-lookup-merge\n",
      "/usr/bin/gmake  -f examples/parallel/CMakeFiles/llama-parallel.dir/build.make examples/parallel/CMakeFiles/llama-parallel.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/parallel /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/parallel /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/DependInfo.cmake --color=\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/parallel/CMakeFiles/llama-parallel.dir/build.make examples/parallel/CMakeFiles/llama-parallel.dir/build\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/main && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/main/CMakeFiles/llama-cli.dir/main.cpp.o -MF CMakeFiles/llama-cli.dir/main.cpp.o.d -o CMakeFiles/llama-cli.dir/main.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/main/main.cpp\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/parallel && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o -MF CMakeFiles/llama-parallel.dir/parallel.cpp.o.d -o CMakeFiles/llama-parallel.dir/parallel.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/parallel/parallel.cpp\n",
      "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/lookup && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-lookup-stats.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\" -o ../../bin/llama-lookup-stats  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 76%] Built target llama-lookup-stats\n",
      "/usr/bin/gmake  -f examples/passkey/CMakeFiles/llama-passkey.dir/build.make examples/passkey/CMakeFiles/llama-passkey.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/passkey /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/passkey /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/passkey/CMakeFiles/llama-passkey.dir/build.make examples/passkey/CMakeFiles/llama-passkey.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/passkey && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o -MF CMakeFiles/llama-passkey.dir/passkey.cpp.o.d -o CMakeFiles/llama-passkey.dir/passkey.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/passkey/passkey.cpp\n",
      "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/parallel && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-parallel.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-parallel.dir/parallel.cpp.o\" -o ../../bin/llama-parallel  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 77%] Built target llama-parallel\n",
      "/usr/bin/gmake  -f examples/perplexity/CMakeFiles/llama-perplexity.dir/build.make examples/perplexity/CMakeFiles/llama-perplexity.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/perplexity /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/perplexity /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/perplexity/CMakeFiles/llama-perplexity.dir/build.make examples/perplexity/CMakeFiles/llama-perplexity.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 77%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/perplexity && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o -MF CMakeFiles/llama-perplexity.dir/perplexity.cpp.o.d -o CMakeFiles/llama-perplexity.dir/perplexity.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/perplexity/perplexity.cpp\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/perplexity/perplexity.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[K/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/perplexity/perplexity.cpp:1736:41:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kparameter passing for argument of type ‘\u001b[01m\u001b[Kstd::pair<double, double>\u001b[m\u001b[K’ when C++17 is enabled changed to match C++14 \u001b]8;;https://gcc.gnu.org/gcc-10/changes.html#empty_base\u0007in GCC 10.1\u001b]8;;\u0007\n",
      " 1736 |             return std::make_pair(0., 0.\u001b[01;36m\u001b[K)\u001b[m\u001b[K;\n",
      "      |                                         \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/passkey && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-passkey.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-passkey.dir/passkey.cpp.o\" -o ../../bin/llama-passkey  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 78%] Built target llama-passkey\n",
      "/usr/bin/gmake  -f examples/quantize/CMakeFiles/llama-quantize.dir/build.make examples/quantize/CMakeFiles/llama-quantize.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/quantize /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/quantize /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/quantize/CMakeFiles/llama-quantize.dir/build.make examples/quantize/CMakeFiles/llama-quantize.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 78%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/quantize && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/quantize/../../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o -MF CMakeFiles/llama-quantize.dir/quantize.cpp.o.d -o CMakeFiles/llama-quantize.dir/quantize.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/quantize/quantize.cpp\n",
      "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/main && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-cli.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-cli.dir/main.cpp.o\" -o ../../bin/llama-cli  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 78%] Built target llama-cli\n",
      "/usr/bin/gmake  -f examples/retrieval/CMakeFiles/llama-retrieval.dir/build.make examples/retrieval/CMakeFiles/llama-retrieval.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/retrieval /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/retrieval /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/retrieval/CMakeFiles/llama-retrieval.dir/build.make examples/retrieval/CMakeFiles/llama-retrieval.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 78%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/retrieval && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o -MF CMakeFiles/llama-retrieval.dir/retrieval.cpp.o.d -o CMakeFiles/llama-retrieval.dir/retrieval.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/retrieval/retrieval.cpp\n",
      "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/quantize && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-quantize.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-quantize.dir/quantize.cpp.o\" -o ../../bin/llama-quantize  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 79%] Built target llama-quantize\n",
      "/usr/bin/gmake  -f examples/server/CMakeFiles/llama-server.dir/build.make examples/server/CMakeFiles/llama-server.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 80%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/server && /usr/bin/cmake -DINPUT=/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/server/public/loading.html -DOUTPUT=/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/server/loading.html.hpp -P /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/scripts/xxd.cmake\n",
      "[ 80%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/server && /usr/bin/cmake -DINPUT=/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/server/public/index.html.gz -DOUTPUT=/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/server/index.html.gz.hpp -P /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/scripts/xxd.cmake\n",
      "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/retrieval && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-retrieval.dir/link.txt --verbose=1\n",
      "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llama-bench && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-bench.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\" -o ../../bin/llama-retrieval  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-bench.dir/llama-bench.cpp.o\" -o ../../bin/llama-bench  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 82%] Built target llama-retrieval\n",
      "/usr/bin/gmake  -f examples/save-load-state/CMakeFiles/llama-save-load-state.dir/build.make examples/save-load-state/CMakeFiles/llama-save-load-state.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/save-load-state /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/save-load-state /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/save-load-state/CMakeFiles/llama-save-load-state.dir/build.make examples/save-load-state/CMakeFiles/llama-save-load-state.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 82%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/save-load-state && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o -MF CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o.d -o CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/save-load-state/save-load-state.cpp\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 82%] Built target llama-bench\n",
      "/usr/bin/gmake  -f examples/run/CMakeFiles/llama-run.dir/build.make examples/run/CMakeFiles/llama-run.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/run /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/run /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/run/CMakeFiles/llama-run.dir/build.make examples/run/CMakeFiles/llama-run.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 82%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/run && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/run/CMakeFiles/llama-run.dir/run.cpp.o -MF CMakeFiles/llama-run.dir/run.cpp.o.d -o CMakeFiles/llama-run.dir/run.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/run/run.cpp\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/server /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/server /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/server/CMakeFiles/llama-server.dir/build.make examples/server/CMakeFiles/llama-server.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 82%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/server && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/server -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/server -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/server/CMakeFiles/llama-server.dir/server.cpp.o -MF CMakeFiles/llama-server.dir/server.cpp.o.d -o CMakeFiles/llama-server.dir/server.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/server/server.cpp\n",
      "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/save-load-state && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-save-load-state.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\" -o ../../bin/llama-save-load-state  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 83%] Built target llama-save-load-state\n",
      "/usr/bin/gmake  -f examples/speculative/CMakeFiles/llama-speculative.dir/build.make examples/speculative/CMakeFiles/llama-speculative.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/speculative /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/speculative /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/speculative/CMakeFiles/llama-speculative.dir/build.make examples/speculative/CMakeFiles/llama-speculative.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 83%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/speculative && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o -MF CMakeFiles/llama-speculative.dir/speculative.cpp.o.d -o CMakeFiles/llama-speculative.dir/speculative.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/speculative/speculative.cpp\n",
      "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/perplexity && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-perplexity.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\" -o ../../bin/llama-perplexity  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 84%] Built target llama-perplexity\n",
      "[ 84%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/run && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o -MF CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o.d -o CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/run/linenoise.cpp/linenoise.cpp\n",
      "/usr/bin/gmake  -f examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/build.make examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/speculative-simple /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/speculative-simple /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/build.make examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 84%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/speculative-simple && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o -MF CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o.d -o CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/speculative-simple/speculative-simple.cpp\n",
      "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/speculative-simple && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-speculative-simple.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\" -o ../../bin/llama-speculative-simple  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 85%] Built target llama-speculative-simple\n",
      "/usr/bin/gmake  -f examples/tokenize/CMakeFiles/llama-tokenize.dir/build.make examples/tokenize/CMakeFiles/llama-tokenize.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/tokenize /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/tokenize /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/tokenize/CMakeFiles/llama-tokenize.dir/build.make examples/tokenize/CMakeFiles/llama-tokenize.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 85%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/tokenize && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o -MF CMakeFiles/llama-tokenize.dir/tokenize.cpp.o.d -o CMakeFiles/llama-tokenize.dir/tokenize.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/tokenize/tokenize.cpp\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/speculative && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-speculative.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-speculative.dir/speculative.cpp.o\" -o ../../bin/llama-speculative  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 86%] Built target llama-speculative\n",
      "/usr/bin/gmake  -f examples/tts/CMakeFiles/llama-tts.dir/build.make examples/tts/CMakeFiles/llama-tts.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/tts /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/tts /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/tts/CMakeFiles/llama-tts.dir/build.make examples/tts/CMakeFiles/llama-tts.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 86%] \u001b[32mBuilding CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/tts && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o -MF CMakeFiles/llama-tts.dir/tts.cpp.o.d -o CMakeFiles/llama-tts.dir/tts.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/tts/tts.cpp\n",
      "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/tokenize && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-tokenize.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\" -o ../../bin/llama-tokenize  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 87%] Built target llama-tokenize\n",
      "/usr/bin/gmake  -f examples/gen-docs/CMakeFiles/llama-gen-docs.dir/build.make examples/gen-docs/CMakeFiles/llama-gen-docs.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gen-docs /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gen-docs /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/gen-docs/CMakeFiles/llama-gen-docs.dir/build.make examples/gen-docs/CMakeFiles/llama-gen-docs.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 88%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gen-docs && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o -MF CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o.d -o CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/gen-docs/gen-docs.cpp\n",
      "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/run && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-run.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-run.dir/run.cpp.o\" \"CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\" -o ../../bin/llama-run  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 89%] Built target llama-run\n",
      "/usr/bin/gmake  -f examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/build.make examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/convert-llama2c-to-ggml /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/convert-llama2c-to-ggml /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/build.make examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 90%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/convert-llama2c-to-ggml && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o -MF CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o.d -o CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp\n",
      "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/gen-docs && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-gen-docs.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\" -o ../../bin/llama-gen-docs  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 90%] Built target llama-gen-docs\n",
      "/usr/bin/gmake  -f examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/build.make examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/cvector-generator /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/cvector-generator /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/build.make examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 91%] \u001b[32mBuilding CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/cvector-generator && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o -MF CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o.d -o CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/cvector-generator/cvector-generator.cpp\n",
      "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/convert-llama2c-to-ggml && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-convert-llama2c-to-ggml.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\" -o ../../bin/llama-convert-llama2c-to-ggml  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 91%] Built target llama-convert-llama2c-to-ggml\n",
      "/usr/bin/gmake  -f examples/export-lora/CMakeFiles/llama-export-lora.dir/build.make examples/export-lora/CMakeFiles/llama-export-lora.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/export-lora /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/export-lora /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/export-lora/CMakeFiles/llama-export-lora.dir/build.make examples/export-lora/CMakeFiles/llama-export-lora.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 92%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/export-lora && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o -MF CMakeFiles/llama-export-lora.dir/export-lora.cpp.o.d -o CMakeFiles/llama-export-lora.dir/export-lora.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/export-lora/export-lora.cpp\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/cvector-generator && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-cvector-generator.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\" -o ../../bin/llama-cvector-generator  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 92%] Built target llama-cvector-generator\n",
      "/usr/bin/gmake  -f examples/llava/CMakeFiles/llama-llava-cli.dir/build.make examples/llava/CMakeFiles/llama-llava-cli.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/llava/CMakeFiles/llama-llava-cli.dir/build.make examples/llava/CMakeFiles/llama-llava-cli.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 93%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/../.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/../../common -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o -MF CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o.d -o CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/llava-cli.cpp\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/export-lora && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-export-lora.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\" -o ../../bin/llama-export-lora  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 93%] Built target llama-export-lora\n",
      "/usr/bin/gmake  -f examples/llava/CMakeFiles/llama-minicpmv-cli.dir/build.make examples/llava/CMakeFiles/llama-minicpmv-cli.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/llava/CMakeFiles/llama-minicpmv-cli.dir/build.make examples/llava/CMakeFiles/llama-minicpmv-cli.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 93%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/../.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/../../common -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o -MF CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o.d -o CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/minicpmv-cli.cpp\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-llava-cli.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o\" CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o -o ../../bin/llama-llava-cli  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 93%] Built target llama-llava-cli\n",
      "/usr/bin/gmake  -f examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/build.make examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/build.make examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 93%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/../.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/../../common -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o -MF CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o.d -o CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/qwen2vl-cli.cpp\n",
      "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-minicpmv-cli.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o\" CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o -o ../../bin/llama-minicpmv-cli  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 94%] Built target llama-minicpmv-cli\n",
      "/usr/bin/gmake  -f examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/build.make examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/build.make examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 94%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/../.. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/../../common -mcpu=native -O3 -DNDEBUG -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o -MF CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o.d -o CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/examples/llava/clip-quantize-cli.cpp\n",
      "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/tts && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-tts.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-tts.dir/tts.cpp.o\" -o ../../bin/llama-tts  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 95%] Built target llama-tts\n",
      "/usr/bin/gmake  -f pocs/vdot/CMakeFiles/llama-vdot.dir/build.make pocs/vdot/CMakeFiles/llama-vdot.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/pocs/vdot /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/pocs/vdot /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f pocs/vdot/CMakeFiles/llama-vdot.dir/build.make pocs/vdot/CMakeFiles/llama-vdot.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 95%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/pocs/vdot && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/pocs -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -MD -MT pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o -MF CMakeFiles/llama-vdot.dir/vdot.cpp.o.d -o CMakeFiles/llama-vdot.dir/vdot.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/pocs/vdot/vdot.cpp\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-clip-quantize-cli\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-llava-clip-quantize-cli.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o\" CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o -o ../../bin/llama-llava-clip-quantize-cli  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 96%] Built target llama-llava-clip-quantize-cli\n",
      "/usr/bin/gmake  -f pocs/vdot/CMakeFiles/llama-q8dot.dir/build.make pocs/vdot/CMakeFiles/llama-q8dot.dir/depend\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/pocs/vdot /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/pocs/vdot /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/DependInfo.cmake --color=\n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/gmake  -f pocs/vdot/CMakeFiles/llama-q8dot.dir/build.make pocs/vdot/CMakeFiles/llama-q8dot.dir/build\n",
      "gmake[2]: Entering directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 96%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/pocs/vdot && /usr/bin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/pocs -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/common/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/. -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../include -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/src/../common -I/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/ggml/src/../include -mcpu=native -O3 -DNDEBUG -MD -MT pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o -MF CMakeFiles/llama-q8dot.dir/q8dot.cpp.o.d -o CMakeFiles/llama-q8dot.dir/q8dot.cpp.o -c /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/pocs/vdot/q8dot.cpp\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/pocs/vdot && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-vdot.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-vdot.dir/vdot.cpp.o\" -o ../../bin/llama-vdot  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/llava && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-qwen2vl-cli.dir/link.txt --verbose=1\n",
      "[ 98%] Built target llama-vdot\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o\" CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o -o ../../bin/llama-qwen2vl-cli  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 98%] Built target llama-qwen2vl-cli\n",
      "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/pocs/vdot && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-q8dot.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\" -o ../../bin/llama-q8dot  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[ 99%] Built target llama-q8dot\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
      "cd /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/examples/server && /usr/bin/cmake -E cmake_link_script CMakeFiles/llama-server.dir/link.txt --verbose=1\n",
      "/usr/bin/c++ -mcpu=native -O3 -DNDEBUG \"CMakeFiles/llama-server.dir/server.cpp.o\" -o ../../bin/llama-server  -Wl,-rpath,/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/bin: ../../common/libcommon.a ../../bin/libllama.so ../../bin/libggml.so ../../bin/libggml-cpu.so ../../bin/libggml-base.so \n",
      "gmake[2]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "[100%] Built target llama-server\n",
      "gmake[1]: Leaving directory '/home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build'\n",
      "/usr/bin/cmake -E cmake_progress_start /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/build/CMakeFiles 0\n"
     ]
    }
   ],
   "source": [
    "!cmake -B llama.cpp/build -S llama.cpp -DCMAKE_CXX_FLAGS=\"-mcpu=native\" -DCMAKE_C_FLAGS=\"-mcpu=native\"\n",
    "!cmake --build llama.cpp/build --config Release -j$(nproc) -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These commands configure and compile the `llama.cpp` project using CMake with optimizations tailored for the Raspberry Pi 5:\n",
    "\n",
    "- **First Command**: \n",
    "  Sets up the build environment by specifying the source directory (`llama.cpp`) and the build directory (`llama.cpp/build`).\n",
    "\n",
    "- **Second Command**: \n",
    "  Compiles the project in `Release` mode, which enables various optimizations for performance, including Arm-specific flags to leverage the Raspberry Pi 5's Cortex-A76 CPU and Neon SIMD instructions.\n",
    "\n",
    "\n",
    "**NOTE** This may take a few minutes to build. Make sure -j is specified in the code cell above to use all processing cores.\n",
    "\n",
    "### 5.C Installing Python Dependencies\n",
    "\n",
    "This command installs the necessary Python dependencies required by `llama.cpp`. The requirements.txt file lists all the Python packages needed for the framework to function correctly. Ensuring these dependencies are installed is crucial for the subsequent Python scripts and conversion tools to operate without issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n",
      "Collecting numpy~=1.26.4 (from -r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 1))\n",
      "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: sentencepiece~=0.2.0 in ./pi5_env/lib/python3.12/site-packages (from -r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.45.1 in ./pi5_env/lib/python3.12/site-packages (from -r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (4.49.0)\n",
      "Collecting gguf>=0.1.0 (from -r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4))\n",
      "  Downloading https://www.piwheels.org/simple/gguf/gguf-0.14.0-py3-none-any.whl (76 kB)\n",
      "Collecting protobuf<5.0.0,>=4.21.0 (from -r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 5))\n",
      "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_aarch64.whl.metadata (541 bytes)\n",
      "Collecting torch~=2.2.1 (from -r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3))\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-2.2.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (86.5 MB)\n",
      "Requirement already satisfied: filelock in ./pi5_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./pi5_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (0.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./pi5_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./pi5_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./pi5_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./pi5_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./pi5_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./pi5_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./pi5_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./pi5_env/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./pi5_env/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./pi5_env/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./pi5_env/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./pi5_env/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2025.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./pi5_env/lib/python3.12/site-packages (from jinja2->torch~=2.2.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./pi5_env/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./pi5_env/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./pi5_env/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./pi5_env/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./pi5_env/lib/python3.12/site-packages (from sympy->torch~=2.2.1->-r /home/oeg1n18/github/Generative_AI_on_arm/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n",
      "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (13.9 MB)\n",
      "Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_aarch64.whl (293 kB)\n",
      "Installing collected packages: protobuf, numpy, torch, gguf\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "Successfully installed gguf-0.14.0 numpy-1.26.4 protobuf-4.25.6 torch-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.D Converting Hugging Face Models to GGUF Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: OpenELM-hf\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F32, shape = {768, 1280}\n",
      "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.float32 --> F32, shape = {1280, 1152}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F32, shape = {1280, 768}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F32, shape = {1280, 768}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F32, shape = {768, 1280}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F32, shape = {768, 1280}\n",
      "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.float32 --> F32, shape = {1280, 1152}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F32, shape = {1280, 1024}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F32, shape = {1280, 1024}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F32, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F32, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.float32 --> F32, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F32, shape = {1280, 3584}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F32, shape = {1280, 3584}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F32, shape = {3584, 1280}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F32, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.float32 --> F32, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F32, shape = {1280, 3840}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F32, shape = {1280, 3840}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F32, shape = {3840, 1280}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F32, shape = {1280, 1280}\n",
      "INFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.float32 --> F32, shape = {1280, 1920}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F32, shape = {1280, 4352}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F32, shape = {1280, 4352}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F32, shape = {4352, 1280}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F32, shape = {1280, 1280}\n",
      "INFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.float32 --> F32, shape = {1280, 1920}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F32, shape = {1280, 4608}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F32, shape = {1280, 4608}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F32, shape = {4608, 1280}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F32, shape = {1280, 1280}\n",
      "INFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.float32 --> F32, shape = {1280, 1920}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F32, shape = {1280, 4864}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F32, shape = {1280, 4864}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F32, shape = {4864, 1280}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F32, shape = {1280, 1280}\n",
      "INFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.float32 --> F32, shape = {1280, 1920}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F32, shape = {1280, 5120}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F32, shape = {1280, 5120}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F32, shape = {5120, 1280}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F32, shape = {768, 1280}\n",
      "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.float32 --> F32, shape = {1280, 1152}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F32, shape = {1280, 1280}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F32, shape = {1280, 1280}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F32, shape = {1280, 1280}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F32, shape = {768, 1280}\n",
      "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.float32 --> F32, shape = {1280, 1152}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F32, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F32, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F32, shape = {1536, 1280}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F32, shape = {768, 1280}\n",
      "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.float32 --> F32, shape = {1280, 1152}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F32, shape = {1280, 1792}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F32, shape = {1280, 1792}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F32, shape = {1792, 1280}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F32, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.float32 --> F32, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F32, shape = {1280, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F32, shape = {1280, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F32, shape = {2048, 1280}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F32, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.float32 --> F32, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F32, shape = {1280, 2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F32, shape = {1280, 2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F32, shape = {2560, 1280}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F32, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.float32 --> F32, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F32, shape = {1280, 2816}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F32, shape = {1280, 2816}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F32, shape = {2816, 1280}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F32, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.float32 --> F32, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F32, shape = {1280, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F32, shape = {1280, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F32, shape = {3072, 1280}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F32, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.float32 --> F32, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F32, shape = {1280, 3328}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F32, shape = {1280, 3328}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F32, shape = {3328, 1280}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.float32 --> F32, shape = {1280, 32000}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "WARNING:hf-to-gguf:Using tokenizer from 'llama.cpp/models/ggml-vocab-llama-spm.gguf'\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:checkpoints/OpenELM-gguf: n_tensors = 146, total_size = 1.1G\n",
      "Writing: 100%|██████████████████████████| 1.09G/1.09G [00:41<00:00, 26.5Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to checkpoints/OpenELM-gguf\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py checkpoints/OpenELM-hf --outfile checkpoints/OpenELM-gguf/ --outtype f32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command converts a Hugging Face (.hf) model checkpoint to the GGUF format used by `llama.cpp`. The convert_hf_to_gguf.py script handles the transformation, taking the original model from the checkpoints/OpenELM-hf directory and saving the converted GGUF model to checkpoints/OpenELM-gguf/.\n",
    "\n",
    "### 5.E Quantizing the Model to INT8 Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: OpenELM-hf\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> Q8_0, shape = {768, 1280}\n",
      "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.float32 --> Q8_0, shape = {1280, 1152}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> Q8_0, shape = {1280, 768}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> Q8_0, shape = {1280, 768}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> Q8_0, shape = {768, 1280}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> Q8_0, shape = {768, 1280}\n",
      "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.float32 --> Q8_0, shape = {1280, 1152}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> Q8_0, shape = {1280, 1024}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> Q8_0, shape = {1280, 1024}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> Q8_0, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> Q8_0, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.float32 --> Q8_0, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> Q8_0, shape = {1280, 3584}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> Q8_0, shape = {1280, 3584}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> Q8_0, shape = {3584, 1280}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> Q8_0, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.float32 --> Q8_0, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> Q8_0, shape = {1280, 3840}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> Q8_0, shape = {1280, 3840}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> Q8_0, shape = {3840, 1280}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> Q8_0, shape = {1280, 1280}\n",
      "INFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.float32 --> Q8_0, shape = {1280, 1920}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> Q8_0, shape = {1280, 4352}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> Q8_0, shape = {1280, 4352}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> Q8_0, shape = {4352, 1280}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> Q8_0, shape = {1280, 1280}\n",
      "INFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.float32 --> Q8_0, shape = {1280, 1920}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> Q8_0, shape = {1280, 4608}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> Q8_0, shape = {1280, 4608}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> Q8_0, shape = {4608, 1280}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> Q8_0, shape = {1280, 1280}\n",
      "INFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.float32 --> Q8_0, shape = {1280, 1920}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> Q8_0, shape = {1280, 4864}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> Q8_0, shape = {1280, 4864}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> Q8_0, shape = {4864, 1280}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> Q8_0, shape = {1280, 1280}\n",
      "INFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.float32 --> Q8_0, shape = {1280, 1920}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> Q8_0, shape = {1280, 5120}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> Q8_0, shape = {1280, 5120}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> Q8_0, shape = {5120, 1280}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> Q8_0, shape = {768, 1280}\n",
      "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.float32 --> Q8_0, shape = {1280, 1152}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> Q8_0, shape = {1280, 1280}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> Q8_0, shape = {1280, 1280}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> Q8_0, shape = {1280, 1280}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> Q8_0, shape = {768, 1280}\n",
      "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.float32 --> Q8_0, shape = {1280, 1152}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> Q8_0, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> Q8_0, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> Q8_0, shape = {1536, 1280}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> Q8_0, shape = {768, 1280}\n",
      "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.float32 --> Q8_0, shape = {1280, 1152}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> Q8_0, shape = {1280, 1792}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> Q8_0, shape = {1280, 1792}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> Q8_0, shape = {1792, 1280}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> Q8_0, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.float32 --> Q8_0, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> Q8_0, shape = {1280, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> Q8_0, shape = {1280, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> Q8_0, shape = {2048, 1280}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> Q8_0, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.float32 --> Q8_0, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> Q8_0, shape = {1280, 2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> Q8_0, shape = {1280, 2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> Q8_0, shape = {2560, 1280}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> Q8_0, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.float32 --> Q8_0, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> Q8_0, shape = {1280, 2816}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> Q8_0, shape = {1280, 2816}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> Q8_0, shape = {2816, 1280}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> Q8_0, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.float32 --> Q8_0, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> Q8_0, shape = {1280, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> Q8_0, shape = {1280, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> Q8_0, shape = {3072, 1280}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> Q8_0, shape = {1024, 1280}\n",
      "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.float32 --> Q8_0, shape = {1280, 1536}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> Q8_0, shape = {1280, 3328}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> Q8_0, shape = {1280, 3328}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> Q8_0, shape = {3328, 1280}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.float32 --> F32, shape = {1280}\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.float32 --> Q8_0, shape = {1280, 32000}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "WARNING:hf-to-gguf:Using tokenizer from 'llama.cpp/models/ggml-vocab-llama-spm.gguf'\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:checkpoints/OpenELM-Q8-gguf: n_tensors = 146, total_size = 288.6M\n",
      "Writing: 100%|████████████████████████████| 289M/289M [00:03<00:00, 83.9Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to checkpoints/OpenELM-Q8-gguf\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py checkpoints/OpenELM-hf --outfile checkpoints/OpenELM-Q8-gguf/ --outtype q8_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command converts and quantizes the Hugging Face model checkpoint to an **INT8** GGUF format using the `--outtype q8_0` flag, which specifies **INT8** symmetric quantization just like we have done in the previous sections. Unlike our previous methods that quantized only feedforward layers, this approach applies quantization to the entire model, including the attention mechanism, offering greater memory savings.\n",
    "\n",
    "\n",
    "### 5.F Measuring Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Model Size FP32 1035.8 (Mb)\n",
      "PyTorch Model Size Int8 524.4 (Mb)\n",
      "GGUF Model Size FP32 1036.5 (Mb)\n",
      "GGUF Model Size Int8 276.0 (Mb)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "int8_size = os.path.getsize('checkpoints/OpenELM-Q8-gguf')\n",
    "fp_size = os.path.getsize('checkpoints/OpenELM-gguf')\n",
    "lammacpp_model_size = fp_size / 1024**2\n",
    "llamacpp_model_size_quantized = int8_size / 1024**2\n",
    "\n",
    "print(f\"PyTorch Model Size FP32 {pytorch_model_size:.1f} (Mb)\")\n",
    "print(f\"PyTorch Model Size Int8 {pytorch_model_size_quantized:.1f} (Mb)\")\n",
    "print(f\"GGUF Model Size FP32 {lammacpp_model_size:.1f} (Mb)\")\n",
    "print(f\"GGUF Model Size Int8 {llamacpp_model_size_quantized:.1f} (Mb)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above Python script calculates and compares the sizes of the full precision and **our** quantized HF model and the GGUF models. You should see that our quantized PyTorch model is 2x smaller than the full precision model, and the GGUF models are 4x smaller than the full precision model. This is because the quantization approach we used in the previous sections only quantized the feedforward layers, not the attention mechanism. The GGUF quantization approached we performed with `llama.cpp` quantizes the entire model, including the attention mechanism, offering greater memory savings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.G Running Inference with the Full Precision Model\n",
    "\n",
    "Next, we can run the model using the `llama.cpp` library with PyTorch bindings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.7.tar.gz (66.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "  Installing build dependencies ... \u001bdone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in ./pi5_env/lib/python3.12/site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in ./pi5_env/lib/python3.12/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached https://www.piwheels.org/simple/diskcache/diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in ./pi5_env/lib/python3.12/site-packages (from llama-cpp-python) (3.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./pi5_env/lib/python3.12/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "done\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.7-cp312-cp312-linux_aarch64.whl size=4218689 sha256=4ce64e603b62c4b9ff3a43b6f03ab03cb7d706aab2c61319e7738df83c150703\n",
      "  Stored in directory: /home/oeg1n18/.cache/pip/wheels/c9/b1/23/8a682c248add4288df3d136a788adcea3df7fdac1bca426799\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 30 key-value pairs and 146 tensors from checkpoints/OpenELM-gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = openelm\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = OpenELM 270M Instruct\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Apple\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = OpenELM\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 270M\n",
      "llama_model_loader: - kv   7:                        openelm.block_count u32              = 16\n",
      "llama_model_loader: - kv   8:                     openelm.context_length u32              = 2048\n",
      "llama_model_loader: - kv   9:                   openelm.embedding_length u32              = 1280\n",
      "llama_model_loader: - kv  10:                openelm.feed_forward_length arr[i32,16]      = [768, 1024, 1280, 1536, 1792, 2048, 2...\n",
      "llama_model_loader: - kv  11:               openelm.attention.head_count arr[i32,16]      = [12, 12, 12, 12, 12, 16, 16, 16, 16, ...\n",
      "llama_model_loader: - kv  12:            openelm.attention.head_count_kv arr[i32,16]      = [3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, ...\n",
      "llama_model_loader: - kv  13:                     openelm.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  14:   openelm.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:               openelm.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  16:               openelm.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  17:             openelm.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 0\n",
      "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  146 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = all F32\n",
      "print_info: file size   = 1.01 GiB (32.00 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = openelm\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 2048\n",
      "print_info: n_embd           = 1280\n",
      "print_info: n_layer          = 16\n",
      "print_info: n_head           = [12, 12, 12, 12, 12, 16, 16, 16, 16, 16, 16, 16, 20, 20, 20, 20]\n",
      "print_info: n_head_kv        = [3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5]\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = [192, 192, 192, 192, 192, 256, 256, 256, 256, 256, 256, 256, 320, 320, 320, 320]\n",
      "print_info: n_embd_v_gqa     = [192, 192, 192, 192, 192, 256, 256, 256, 256, 256, 256, 256, 320, 320, 320, 320]\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = [768, 1024, 1280, 1536, 1792, 2048, 2560, 2816, 3072, 3328, 3584, 3840, 4352, 4608, 4864, 5120]\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 2048\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 270M\n",
      "print_info: model params     = 271.53 M\n",
      "print_info: general.name     = OpenELM 270M Instruct\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (f32) (and 146 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  1035.79 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (2048) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 192, n_embd_v_gqa = 192\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 192, n_embd_v_gqa = 192\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 192, n_embd_v_gqa = 192\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 192, n_embd_v_gqa = 192\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 192, n_embd_v_gqa = 192\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 320, n_embd_v_gqa = 320\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 320, n_embd_v_gqa = 320\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 320, n_embd_v_gqa = 320\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 320, n_embd_v_gqa = 320\n",
      "llama_kv_cache_init:        CPU KV buffer size =     7.88 MiB\n",
      "llama_init_from_model: KV self size  =    7.88 MiB, K (f16):    3.94 MiB, V (f16):    3.94 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =    68.51 MiB\n",
      "llama_init_from_model: graph nodes  = 646\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '0', 'general.architecture': 'openelm', 'openelm.attention.layer_norm_rms_epsilon': '0.000001', 'general.basename': 'OpenELM', 'tokenizer.ggml.add_eos_token': 'false', 'openelm.context_length': '2048', 'tokenizer.ggml.pre': 'default', 'general.name': 'OpenELM 270M Instruct', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '270M', 'general.organization': 'Apple', 'openelm.block_count': '16', 'openelm.attention.key_length': '64', 'openelm.embedding_length': '1280', 'openelm.rope.freq_base': '10000.000000', 'openelm.attention.value_length': '64', 'openelm.rope.dimension_count': '64'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =     689.45 ms\n",
      "llama_perf_context_print: prompt eval time =     689.17 ms /     8 tokens (   86.15 ms per token,    11.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3108.88 ms /    29 runs   (  107.20 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3808.97 ms /    37 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import time \n",
    "\n",
    "llm = Llama(\n",
    "      model_path=\"checkpoints/OpenELM-gguf\",\n",
    ")\n",
    "\n",
    "st = time.time()\n",
    "output = llm(\n",
    "      \"Arm is a company that designs\", # Prompt\n",
    "      max_tokens=30, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      temperature=0,\n",
    "      top_p=1.0,  # Ensures that all tokens are considered (no sampling)\n",
    "      top_k=0,  # No top-k sampling; all tokens are considered for selection\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") \n",
    "et = time.time()\n",
    "\n",
    "llamacpp_generation_latency = (et - st) * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you scroll to the bottom of this output cell, you should be able to see which intrinsics are enabled in the `llama.cpp` backend. For instance, you should see `| NEON = 1|` showing that the 128-bit SIMD vector registers are being utilized. Additionally you should see `| Arm_FMA = 1|` showing that the fused multiply-add instruction is being used. These are heavily utilised for accumulation in matrix multiplications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arm is a company that designs and manufactures microcontrollers, which are tiny electronic devices that can perform specific functions. The company has been around since the 1970'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should also see that this generated output is identical to the full precision Hugging Face model, indicating that the GGUF conversion and inference has been performed correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.H Running Inference with the Quantized INT8 Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 30 key-value pairs and 146 tensors from checkpoints/OpenELM-Q8-gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = openelm\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = OpenELM 270M Instruct\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Apple\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = OpenELM\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 270M\n",
      "llama_model_loader: - kv   7:                        openelm.block_count u32              = 16\n",
      "llama_model_loader: - kv   8:                     openelm.context_length u32              = 2048\n",
      "llama_model_loader: - kv   9:                   openelm.embedding_length u32              = 1280\n",
      "llama_model_loader: - kv  10:                openelm.feed_forward_length arr[i32,16]      = [768, 1024, 1280, 1536, 1792, 2048, 2...\n",
      "llama_model_loader: - kv  11:               openelm.attention.head_count arr[i32,16]      = [12, 12, 12, 12, 12, 16, 16, 16, 16, ...\n",
      "llama_model_loader: - kv  12:            openelm.attention.head_count_kv arr[i32,16]      = [3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, ...\n",
      "llama_model_loader: - kv  13:                     openelm.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  14:   openelm.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:               openelm.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  16:               openelm.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  17:             openelm.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:   81 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 275.26 MiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = openelm\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 2048\n",
      "print_info: n_embd           = 1280\n",
      "print_info: n_layer          = 16\n",
      "print_info: n_head           = [12, 12, 12, 12, 12, 16, 16, 16, 16, 16, 16, 16, 20, 20, 20, 20]\n",
      "print_info: n_head_kv        = [3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5]\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = [192, 192, 192, 192, 192, 256, 256, 256, 256, 256, 256, 256, 320, 320, 320, 320]\n",
      "print_info: n_embd_v_gqa     = [192, 192, 192, 192, 192, 256, 256, 256, 256, 256, 256, 256, 320, 320, 320, 320]\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = [768, 1024, 1280, 1536, 1792, 2048, 2560, 2816, 3072, 3328, 3584, 3840, 4352, 4608, 4864, 5120]\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 2048\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 270M\n",
      "print_info: model params     = 271.53 M\n",
      "print_info: general.name     = OpenELM 270M Instruct\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 146 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =   275.26 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (2048) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 192, n_embd_v_gqa = 192\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 192, n_embd_v_gqa = 192\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 192, n_embd_v_gqa = 192\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 192, n_embd_v_gqa = 192\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 192, n_embd_v_gqa = 192\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 320, n_embd_v_gqa = 320\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 320, n_embd_v_gqa = 320\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 320, n_embd_v_gqa = 320\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 320, n_embd_v_gqa = 320\n",
      "llama_kv_cache_init:        CPU KV buffer size =     7.88 MiB\n",
      "llama_init_from_model: KV self size  =    7.88 MiB, K (f16):    3.94 MiB, V (f16):    3.94 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =    68.51 MiB\n",
      "llama_init_from_model: graph nodes  = 646\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7', 'general.architecture': 'openelm', 'openelm.attention.layer_norm_rms_epsilon': '0.000001', 'general.basename': 'OpenELM', 'tokenizer.ggml.add_eos_token': 'false', 'openelm.context_length': '2048', 'tokenizer.ggml.pre': 'default', 'general.name': 'OpenELM 270M Instruct', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '270M', 'general.organization': 'Apple', 'openelm.block_count': '16', 'openelm.attention.key_length': '64', 'openelm.embedding_length': '1280', 'openelm.rope.freq_base': '10000.000000', 'openelm.attention.value_length': '64', 'openelm.rope.dimension_count': '64'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =     169.11 ms\n",
      "llama_perf_context_print: prompt eval time =     168.93 ms /     8 tokens (   21.12 ms per token,    47.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =     790.82 ms /    29 runs   (   27.27 ms per token,    36.67 tokens per second)\n",
      "llama_perf_context_print:       total time =     967.79 ms /    37 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import time \n",
    "\n",
    "llm = Llama(\n",
    "      model_path=\"checkpoints/OpenELM-Q8-gguf\",\n",
    ")\n",
    "\n",
    "st = time.time()\n",
    "output = llm(\n",
    "      \"Arm is a company that designs\", # Prompt\n",
    "      max_tokens=30, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      temperature=0,\n",
    "      top_p=1.0,  # Ensures that all tokens are considered (no sampling)\n",
    "      top_k=0,  # No top-k sampling; all tokens are considered for selection\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") \n",
    "et = time.time()\n",
    "\n",
    "llamacpp_generation_latency_quantized = (et - st) * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should again see at the bottom of the output cell which intrinsics are being used. Notice that with the `int8` model `| MATMUL_INT8 = 0|` indicating that the matrix multiplication is not being performed using `int8` instructions. This is not a mistake but rather because the quantization approach we selected is weight-only quantization, so dequantization occurs before the operations like matrix multiplication. We saw this in Section 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arm is a company that designs and manufactures microcontrollers, which are tiny electronic devices that can perform specific functions. Arm's flagship chip, the Cortex-\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the generation latencies, to see the performance gains from using a lightweight inference model in graph execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Precision Eager Execution (HuggingFace) - 23.89 (s)\n",
      "Full Precision Graph-Mode Execution (Llamma.cpp) - 3.81 (s)\n",
      "INT8 Quantized Graph-Mode Execution (Llama.cpp) - 0.97 (s)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Full Precision Eager Execution (HugginFace) - {pytorch_generation_latency/1000:.2f} (s)\")\n",
    "print(f\"Full Precision Graph-Mode Execution (Llamma.cpp) - {llamacpp_generation_latency/1000:.2f} (s)\")\n",
    "print(f\"INT8 Quantized Graph-Mode Execution (Llama.cpp) - {llamacpp_generation_latency_quantized/1000:.2f} (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Your output should look something like this \n",
    "\n",
    "**Full Precision Eager Execution (HugginFace) - 22.48 (s)** <br>\n",
    "**Full Precision Graph-Mode Execution (Llamma.cpp) - 3.33 (s)** <br>\n",
    "**INT8 Quantized Graph-Mode Execution (Llama.cpp) - 1.55 (s)** <br>\n",
    "\n",
    "This demonstrates the significant advantages of transitioning from a general-purpose, eager execution framework like Hugging Face (built on PyTorch) to a highly optimized, graph-mode execution framework. The results highlight the drastic performance gains achievable, especially when leveraging `int8` quantization for inference.\n",
    "\n",
    "---\n",
    "# Summary\n",
    "In this lab, we focused on optimizing the core operation of AI, matrix multiplication, using Arm's SIMD Neon capabilities. You learned how to benchmark these operations and explored their implementation in high-level libraries such as PyTorch, which simplifies the underlying complexity.\n",
    "\n",
    "In the second part of the lab, we introduced weight-only symmetric quantization, covering both per-tensor and per-channel approaches. You implemented these methods from scratch to reduce the memory consumption of a state-of-the-art large language model. Additionally, you explored how to use the lightweight inference framework `llama.cpp` to deploy the optimized model on your device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-compute the output of the linear layer before quantization. \n",
    "y = torch.matmul(X_fp, W_fp.t()) \n",
    "\n",
    "# Quantize the layer weight 'W'\n",
    "b = 8  # Bit-width for quantization\n",
    "\n",
    "# ------ Complete the missing line here ------\n",
    "# You need to compute a different scale factor for each channel in the weight matrix. \n",
    "# This will result in a scale 'S' vector with the same number of output channels as the weight matrix. \n",
    "S_per_channel = W_fp.abs().max(dim=1, keepdim=True)[0] / (2**(b-1) - 1)  # Scale factor\n",
    "# ---------------------------------------------\n",
    "Q = torch.round(W_fp / S_per_channel).clamp(-2**(b-1), 2**(b-1) - 1)  # Q <- quantize(W)\n",
    "\n",
    "# Weight-only quantized linear layer\n",
    "y_hat_per_channel = torch.matmul(X_fp, (Q * S_per_channel).t()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WeightOnlyInt8Linear(nn.Module): \n",
    "    def __init__(self, in_features, out_features, bias=True): \n",
    "        super().__init__()\n",
    "        self.in_features = in_features # number of input channels \n",
    "        self.out_features = out_features # number of output channels \n",
    "        self.scale = nn.Parameter(torch.ones(1, dtype=torch.float32), requires_grad=False)  # scale factor used for quantizing weights\n",
    "        # Create a tensor with the desired dtype first, then wrap it with nn.Parameter\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features, dtype=torch.int8), requires_grad=False)  # quantized weights\n",
    "        if bias: \n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features)) # bias term\n",
    "        else: \n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def quantize(self, tensor):\n",
    "        b = 8\n",
    "        qmin = -2**(b-1)\n",
    "        qmax = 2**(b-1) - 1\n",
    "        \n",
    "        # ------ Complete the missing line here ------\n",
    "        # you will need to compute a variable called `scale` that is used to quantize the `tensor` Look in the above exmple if you are unsure how to do this. \n",
    "        scale = torch.Tensor((tensor.abs().max(dim=1, keepdims=True)[0] / qmax))\n",
    "        # ---------------------------------------------\n",
    "        scale = scale.clamp(min=1e-8)\n",
    "        quantized_weights = torch.round(tensor / scale).clamp(qmin, qmax)\n",
    "        quantized_weights = quantized_weights.type(torch.int8)\n",
    "        self.weight.data = quantized_weights \n",
    "        self.scale.data = scale\n",
    "        assert self.weight.dtype == torch.int8\n",
    "        return None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ------ Complete the missing line here ------\n",
    "        # You will need to dequantize the weights `self.weight` using the scale factor `self.scale` and compute the output of the linear layer y. \n",
    "        # You can use the `torch.nn.functional.linear` function to perform the linear layer operation. \n",
    "        y = F.linear(x, self.weight * self.scale, self.bias)\n",
    "        # ---------------------------------------------\n",
    "        return y\n",
    "    \n",
    "    def __repr__(self): \n",
    "        return f\"WeightOnlyInt8Linear(in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
